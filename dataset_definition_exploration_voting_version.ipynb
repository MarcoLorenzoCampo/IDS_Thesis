{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1725,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:28.375672400Z",
     "start_time": "2024-02-27T20:18:27.509325Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import RandomUnderSampler as under_sam\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1726,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:28.921302400Z",
     "start_time": "2024-02-27T20:18:28.349328300Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the train set\n",
    "df_train = pd.read_csv('EvalResources/KDDTrain+.txt', sep=\",\", header=None, skipinitialspace = True)\n",
    "df_train = df_train[df_train.columns[:-1]]  # tags column\n",
    "titles = pd.read_csv('EvalResources/Field Names.csv', header=None, skipinitialspace = True)\n",
    "label = pd.Series(['label'], index=[41])\n",
    "titles = pd.concat([titles[0], label])\n",
    "df_train.columns = titles.to_list()\n",
    "df_train = df_train.drop(['num_outbound_cmds'],axis=1)\n",
    "df_train_original = df_train\n",
    "\n",
    "# df_train_original.to_csv('KB Process/NSL-KDD Original Datasets/KDDTrain+_with_labels.txt', index=False)\n",
    "\n",
    "#df_train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1727,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.079451Z",
     "start_time": "2024-02-27T20:18:28.909276900Z"
    }
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "df_test = pd.read_csv('EvalResources/KDDTest+.txt', sep=\",\", header=None, skipinitialspace = True)\n",
    "df_test_ = df_test.sort_index(axis=1)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "df_test_original = df_test\n",
    "\n",
    "# df_test_original.to_csv('KB Process/NSL-KDD Original Datasets/KDDTest+.txt', index=False)\n",
    "\n",
    "#df_test_original"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Execution Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1728,
   "outputs": [],
   "source": [
    "EXPORT_MODELS = 1\n",
    "EXPORT_DATASETS = 0\n",
    "EXPORT_PCA = 0\n",
    "EXPORT_ENCODERS = 0\n",
    "\n",
    "pd.options.display.max_columns = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.086510700Z",
     "start_time": "2024-02-27T20:18:29.014797700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1729,
   "outputs": [],
   "source": [
    "# list of single attacks \n",
    "dos_attacks = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop', 'worm', 'apache2', 'mailbomb', 'processtable', 'udpstorm']\n",
    "probe_attacks = ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']\n",
    "r2l_attacks = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster',\n",
    "                'snmpguess', 'spy', 'warezclient', 'httptunnel', 'named', 'sendmail', 'snmpgetattack', 'xlock', 'xsnoop']\n",
    "u2r_attacks = ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm'] \n",
    "\n",
    "# list of attack classes split according to detection layer\n",
    "dos_probe_list = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop', 'ipsweep', 'nmap', 'portsweep', 'satan']\n",
    "dos_probe_test = ['apache2', 'mailbomb', 'processtable', 'udpstorm', 'mscan', 'saint']\n",
    "u2r_r2l_list = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster',\n",
    "                'snmpguess', 'spy', 'warezclient', 'buffer_overflow', 'loadmodule', 'rootkit', 'perl']\n",
    "u2r_r2l_test = ['httptunnel', 'named', 'sendmail', 'snmpgetattack', 'xlock', 'xsnoop', 'ps', 'xterm', 'sqlattack']\n",
    "normal_list = ['normal']\n",
    "categorical_features = ['protocol_type', 'service', 'flag']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.161301200Z",
     "start_time": "2024-02-27T20:18:29.028071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1730,
   "outputs": [],
   "source": [
    "# load the features obtained with ICFS for both layer 1 and layer 2\n",
    "with open('KBProcess/AWS Downloads/MinimalFeatures/NSL_features_l1.txt', 'r') as f:\n",
    "    common_features_l1 = f.read().split(',')\n",
    "\n",
    "with open('KBProcess/AWS Downloads/MinimalFeatures/NSL_features_l2.txt', 'r') as f:\n",
    "    common_features_l2 = f.read().split(',')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.166800Z",
     "start_time": "2024-02-27T20:18:29.046390800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1731,
   "outputs": [],
   "source": [
    "df_train_and_validate = copy.deepcopy(df_train_original)\n",
    "df_test = copy.deepcopy(df_test_original)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.181923300Z",
     "start_time": "2024-02-27T20:18:29.052926800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1732,
   "outputs": [],
   "source": [
    "# Save all the targets for the dataset\n",
    "\n",
    "y_test_l1 = [1 if x in (dos_attacks+probe_attacks) else 0 for x in df_test['label']]\n",
    "y_test_l2 = [1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_test['label']]\n",
    "\n",
    "if EXPORT_DATASETS:\n",
    "    np.save(\"EvalResources/AdditionalSets/l1_full_test_targets.npy\", y_test_l1)\n",
    "    np.save(\"EvalResources/AdditionalSets/l2_full_test_targets.npy\", y_test_l2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.222635500Z",
     "start_time": "2024-02-27T20:18:29.073225700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOS PROBE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1733,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.399526300Z",
     "start_time": "2024-02-27T20:18:29.105390Z"
    }
   },
   "outputs": [],
   "source": [
    "# split in test and validation set for BOTH layers\n",
    "df_train_original, df_val_original = train_test_split(df_train_and_validate, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataframes specifically for layer 1\n",
    "df_train = copy.deepcopy(df_train_original)\n",
    "df_val = copy.deepcopy(df_val_original)\n",
    "df_test = copy.deepcopy(df_test_original)\n",
    "\n",
    "# target variables for all layers\n",
    "y_train_full = np.array([1 if x not in normal_list else 0 for x in df_train['label']])\n",
    "y_test_full = np.array([1 if x not in normal_list else 0 for x in df_test['label']])\n",
    "\n",
    "# set the target variables accordingly\n",
    "y_train = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_train['label']])\n",
    "y_validate = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_val['label']])\n",
    "y_test = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_test ['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1734,
   "outputs": [],
   "source": [
    "# this dataframe contains the whole train set \n",
    "df_train = df_train.drop(['label'],axis=1)\n",
    "df_train = df_train.reset_index().drop(['index'], axis=1)\n",
    "#df_train"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.452286200Z",
     "start_time": "2024-02-27T20:18:29.267345500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1735,
   "outputs": [],
   "source": [
    "# this dataframe contains the whole validation set\n",
    "df_val = df_val.drop(['label'],axis=1)\n",
    "df_val = df_val.reset_index().drop(['index'], axis=1)\n",
    "#df_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.476802800Z",
     "start_time": "2024-02-27T20:18:29.330892100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1736,
   "outputs": [],
   "source": [
    "# this dataframe contains the whole test set\n",
    "df_test = df_test.drop(['label'],axis=1)\n",
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "#df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.480593100Z",
     "start_time": "2024-02-27T20:18:29.349670300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using features obtained with a random forest on numerical features only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# List of feature names\n",
    "feature_names_l1 = ['src_bytes', 'logged_in', 'count', 'srv_count', 'srv_serror_rate',\n",
    "       'same_srv_rate', 'diff_srv_rate', 'dst_host_srv_count',\n",
    "       'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "       'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "       'dst_host_srv_rerror_rate']\n",
    "\n",
    "# Selecting features using loc\n",
    "X_train = df_train.loc[:, feature_names_l1]\n",
    "X_validate = df_val.loc[:, feature_names_l1]\n",
    "X_test = df_test.loc[:, feature_names_l1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1737,
   "outputs": [],
   "source": [
    "# Using the features extracted by using the ICFS algorithm\n",
    "X_train = df_train[common_features_l1]\n",
    "X_validate = df_val[common_features_l1]\n",
    "X_test = df_test[common_features_l1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.498114100Z",
     "start_time": "2024-02-27T20:18:29.363636700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1738,
   "outputs": [],
   "source": [
    "# 2 one-hot encoders, one for the features of layer1 and one for the features of layer2\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe2 = OneHotEncoder(handle_unknown='ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.518553100Z",
     "start_time": "2024-02-27T20:18:29.380323100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1739,
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.522762Z",
     "start_time": "2024-02-27T20:18:29.388445200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1740,
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.690956200Z",
     "start_time": "2024-02-27T20:18:29.522762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1741,
   "outputs": [],
   "source": [
    "# scaling the train set for layer1\n",
    "df_minmax = scaler1.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(df_minmax, columns=X_train.columns)\n",
    "\n",
    "#X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.775825100Z",
     "start_time": "2024-02-27T20:18:29.674182300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1742,
   "outputs": [],
   "source": [
    "# scaling the validation set for layer1\n",
    "df_minmax_val = scaler1.transform(X_validate)\n",
    "X_validate = pd.DataFrame(df_minmax_val, columns=X_validate.columns)\n",
    "\n",
    "#X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.853492400Z",
     "start_time": "2024-02-27T20:18:29.746690700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1743,
   "outputs": [],
   "source": [
    "# scaling the test set for layer1\n",
    "df_minmax_test = scaler1.transform(X_test)\n",
    "X_test = pd.DataFrame(df_minmax_test, columns=X_test.columns)\n",
    "\n",
    "#X_vtest"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:29.932103900Z",
     "start_time": "2024-02-27T20:18:29.841844700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1744,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the train set\n",
    "label_enc = ohe.fit_transform(df_train[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_train = pd.concat([X_train, df_enc], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:30.172321300Z",
     "start_time": "2024-02-27T20:18:29.922340700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1745,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the validation set\n",
    "label_enc = ohe.transform(df_val[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_validate = pd.concat([X_validate, df_enc], axis=1)\n",
    "\n",
    "#X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:30.222914Z",
     "start_time": "2024-02-27T20:18:30.095642400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1746,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the test set\n",
    "label_enc = ohe.transform(df_test[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test = pd.concat([X_test, df_enc], axis=1)\n",
    "\n",
    "#X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:30.275269700Z",
     "start_time": "2024-02-27T20:18:30.162212300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1747,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:30.280405800Z",
     "start_time": "2024-02-27T20:18:30.186709300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the whole train set:  (100778, 101)\n",
      "Shape of its targets:  (100778,)\n",
      "Shape of the whole train set:  (25195, 101)\n",
      "Shape of its targets:  (25195,)\n",
      "Shape of the whole test set:  (22544, 101)\n",
      "Shape of its targets:  (22544,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the whole train set: ', X_train.shape)\n",
    "print('Shape of its targets: ', y_train.shape)\n",
    "print('Shape of the whole train set: ', X_validate.shape)\n",
    "print('Shape of its targets: ', y_validate.shape)\n",
    "print('Shape of the whole test set: ', X_test.shape)\n",
    "print('Shape of its targets: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export the dataset for training layer 1\n",
    "if EXPORT_DATASETS:\n",
    "    \"\"\"\n",
    "    X_train.to_csv('EvalResources/ProcessedDatasets/x_train_l1.txt', index=False)\n",
    "    X_validate.to_csv('EvalResources/ProcessedDatasets/x_val_l1.txt', index=False)\n",
    "    \"\"\"\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_train_l1', y_train)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_val_l1', y_validate)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_test_l1', y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Principal Component Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1748,
   "outputs": [
    {
     "data": {
      "text/plain": "(100778, 28)"
     },
     "execution_count": 1748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_dos_probe = PCA(n_components=0.95)\n",
    "X_train_dos_probe = pca_dos_probe.fit_transform(X_train)\n",
    "X_test_dos_probe = pca_dos_probe.transform(X_test)\n",
    "X_validate_dos_probe = pca_dos_probe.transform(X_validate)\n",
    "\n",
    "# X_train = X_train.sort_index(axis=1)\n",
    "X_train_dos_probe.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:32.017113100Z",
     "start_time": "2024-02-27T20:18:30.191871300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "if EXPORT_PCA:\n",
    "    # save the pca transformed as well as the transformer\n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_dos_probe.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_test_dos_probe, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedWithPCA/x_test_l1_pca.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_train_dos_probe.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_train_dos_probe, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedWithPCA/x_train_l1_pca.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_validate_dos_probe.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_validate_dos_probe, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedWithPCA/x_val_l1_pca.txt', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building the classifier for the layer1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1749,
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:32.075471100Z",
     "start_time": "2024-02-27T20:18:31.986470500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1750,
   "outputs": [],
   "source": [
    "# Voting classifiers\n",
    "\n",
    "voting_classifiers = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:32.191405400Z",
     "start_time": "2024-02-27T20:18:31.990254700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1751,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for layer 1:\n",
      "Confusion matrix: [TP FN / FP TN]\n",
      " [[12386   277]\n",
      " [ 2315  7566]]\n",
      "Accuracy =  0.8850248403122782\n",
      "F1 Score =  0.8537576167907921\n",
      "Precision =  0.9646818819329338\n",
      "Recall =  0.7657119724724218\n",
      "Train time =  0:00:01.957621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Using HistGradientBoosting classifier\n",
    "dos_probe_classifier = HistGradientBoostingClassifier()\n",
    "\n",
    "start = datetime.now()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "\n",
    "print('Metrics for layer 1:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "\n",
    "voting_classifiers.append((\"hgbc\", dos_probe_classifier))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.085320500Z",
     "start_time": "2024-02-27T20:18:32.041229700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using Random Forest Classifier\n",
    "dos_probe_classifier = RandomForestClassifier()\n",
    "\n",
    "start = datetime.now()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "\n",
    "print('Metrics for layer 1:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "\n",
    "voting_classifiers.append((\"rf\", dos_probe_classifier))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using the Naive Bayes Classifier\n",
    "dos_probe_classifier = GaussianNB()\n",
    "\n",
    "start = datetime.now()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "\n",
    "print('Metrics for layer 1:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "print('Shape of the train set for l1: ', X_train_dos_probe.shape)\n",
    "\n",
    "voting_classifiers.append((\"nbc\", dos_probe_classifier))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Voting classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "dos_probe_classifier = VotingClassifier(estimators=voting_classifiers, voting='soft')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "start = datetime.now()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "\n",
    "print('Using a voting classifier:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "print('Shape of the train set for l1: ', X_train_dos_probe.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2L+U2R classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1752,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.193491Z",
     "start_time": "2024-02-27T20:18:34.086324500Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = copy.deepcopy(df_train_original)\n",
    "df_test = copy.deepcopy(df_test_original)\n",
    "df_val = copy.deepcopy(df_val_original)\n",
    "\n",
    "# load targeted attacks (Normal + r2l + u2r)\n",
    "df_train = df_train[df_train['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "df_val = df_val[df_val['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "df_test = df_test[df_test['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "\n",
    "# set the target variables accordingly\n",
    "y_train = np.array([1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_train['label']])\n",
    "y_val = np.array([1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_val['label']])\n",
    "y_test = np.array([1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_test['label']])\n",
    "\n",
    "df_train = df_train.drop(['label'],axis=1)\n",
    "df_train = df_train.reset_index().drop(['index'], axis=1)\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1753,
   "outputs": [],
   "source": [
    "df_val = df_val.drop(['label'],axis=1)\n",
    "df_val = df_val.reset_index().drop(['index'], axis=1)\n",
    "#df_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.215168200Z",
     "start_time": "2024-02-27T20:18:34.195492700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1754,
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['label'],axis=1)\n",
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "#df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.265967100Z",
     "start_time": "2024-02-27T20:18:34.211176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1755,
   "outputs": [],
   "source": [
    "X_train = df_train\n",
    "X_validate = df_val\n",
    "X_test = df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.270876Z",
     "start_time": "2024-02-27T20:18:34.236604900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using features obtained with a random forest on numerical features only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# List of feature names\n",
    "feature_names_l2 = ['count', 'is_guest_login', 'srv_count', 'hot', 'dst_host_serror_rate',\n",
    "       'dst_host_srv_count', 'dst_host_count', 'dst_host_same_srv_rate',\n",
    "       'dst_host_same_src_port_rate', 'num_file_creations', 'diff_srv_rate']\n",
    "\n",
    "# Selecting features using loc\n",
    "X_train = df_train.loc[:, feature_names_l2]\n",
    "X_validate = df_val.loc[:, feature_names_l2]\n",
    "X_test = df_test.loc[:, feature_names_l2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1756,
   "outputs": [],
   "source": [
    "X_train = df_train[common_features_l2]\n",
    "X_validate = df_val[common_features_l2]\n",
    "X_test = df_test[common_features_l2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.299362Z",
     "start_time": "2024-02-27T20:18:34.250475300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1757,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.357928900Z",
     "start_time": "2024-02-27T20:18:34.261157300Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax = scaler2.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(df_minmax, columns=X_train.columns)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1758,
   "outputs": [],
   "source": [
    "df_minmax = scaler2.transform(X_validate)\n",
    "X_validate = pd.DataFrame(df_minmax, columns=X_validate.columns)\n",
    "#X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.368159900Z",
     "start_time": "2024-02-27T20:18:34.281042600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1759,
   "outputs": [],
   "source": [
    "df_minmax = scaler2.transform(X_test)\n",
    "X_test = pd.DataFrame(df_minmax, columns=X_test.columns)\n",
    "#X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.372913Z",
     "start_time": "2024-02-27T20:18:34.290584500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1760,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.401738900Z",
     "start_time": "2024-02-27T20:18:34.310427200Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the train set\n",
    "label_enc = ohe2.fit_transform(df_train[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_train = pd.concat([X_train, df_enc], axis=1)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1761,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the validation set\n",
    "label_enc = ohe2.transform(df_val[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_validate = pd.concat([X_validate, df_enc], axis=1)\n",
    "#X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.458124500Z",
     "start_time": "2024-02-27T20:18:34.366030500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1762,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the test set\n",
    "label_enc = ohe2.transform(df_test[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test = pd.concat([X_test, df_enc], axis=1)\n",
    "#X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.499108200Z",
     "start_time": "2024-02-27T20:18:34.389439Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1763,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.503369Z",
     "start_time": "2024-02-27T20:18:34.417752200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train set:  (54733, 51)\n",
      "Shape of its target:  (54733,)\n",
      "Shape of the test set:  (12663, 51)\n",
      "Shape of its target:  (12663,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the train set: ', X_train.shape)\n",
    "print('Shape of its target: ', y_train.shape)\n",
    "print('Shape of the test set: ', X_test.shape)\n",
    "print('Shape of its target: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1764,
   "outputs": [],
   "source": [
    "# Under sampling the train set for l2\n",
    "sm = under_sam(sampling_strategy=1)\n",
    "X_train, y_train = sm.fit_resample(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.509182200Z",
     "start_time": "2024-02-27T20:18:34.420772600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export the datasets\n",
    "Train set has been scaled, one hot encoded, undersampled\n",
    "Test set has been scaled and one hot encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export the dataset for training layer 2\n",
    "if EXPORT_DATASETS:\n",
    "    # X_train.to_csv('EvalResources/ProcessedDatasets/x_train_l2.txt', index=False)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_train_l2', y_train)\n",
    "    # X_validate.to_csv('EvalResources/ProcessedDatasets/x_val_l2.txt', index=False)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_val_l2', y_val)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_test_l2', y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1765,
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "pca_r2l_u2r = PCA(n_components=0.95)\n",
    "X_train_r2l_u2r = pca_r2l_u2r.fit_transform(X_train)\n",
    "X_test_r2l_u2r = pca_r2l_u2r.transform(X_test)\n",
    "X_validate_r2l_u2r = pca_r2l_u2r.transform(X_validate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:34.598168500Z",
     "start_time": "2024-02-27T20:18:34.454012900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using Random Forest Classifier\n",
    "r2l_u2r_classifier = RandomForestClassifier()\n",
    "\n",
    "start = datetime.now()\n",
    "r2l_u2r_classifier.fit(X_train_r2l_u2r, y_train)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = r2l_u2r_classifier.predict(X_test_r2l_u2r)\n",
    "\n",
    "print('Metrics for layer 2:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "print('Shape of the training set: ', X_train_r2l_u2r.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1766,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for layer 2:\n",
      "Confusion matrix: [TP FN / FP TN]\n",
      " [[9092  619]\n",
      " [1550 1402]]\n",
      "Accuracy =  0.8287135749822317\n",
      "F1 Score =  0.5638447617132515\n",
      "Precision =  0.6937159821870361\n",
      "Recall =  0.4749322493224932\n",
      "Matthew corr =  0.47472053754163585\n",
      "Train time =  0:00:00.523809\n",
      "Shape of the training set:  (1624, 13)\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine for layer l2\n",
    "r2l_u2r_classifier = SVC(C=0.1, gamma=0.01, kernel='rbf', probability=True)\n",
    "\n",
    "#r2l_u2r_classifier = SVC()\n",
    "\n",
    "start = datetime.now()\n",
    "r2l_u2r_classifier.fit(X_train_r2l_u2r, y_train)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = r2l_u2r_classifier.predict(X_test_r2l_u2r)\n",
    "\n",
    "print('Metrics for layer 2:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "print('Shape of the training set: ', X_train_r2l_u2r.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:36.263312700Z",
     "start_time": "2024-02-27T20:18:34.505527Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1767,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for layer 2:\n",
      "Confusion matrix: [TP FN / FP TN]\n",
      " [[694 118]\n",
      " [  5 807]]\n",
      "Accuracy =  0.9242610837438424\n",
      "F1 Score =  0.9291882556131261\n",
      "Precision =  0.8724324324324324\n",
      "Recall =  0.9938423645320197\n",
      "Matthew corr =  0.8568597990061858\n"
     ]
    }
   ],
   "source": [
    "predicted = r2l_u2r_classifier.predict(X_train_r2l_u2r)\n",
    "\n",
    "print('Metrics for layer 2:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_train,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_train,predicted))\n",
    "print('F1 Score = ', f1_score(y_train,predicted))\n",
    "print('Precision = ', precision_score(y_train,predicted))\n",
    "print('Recall = ', recall_score(y_train,predicted))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_train,predicted))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:36.439703Z",
     "start_time": "2024-02-27T20:18:36.264322200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "if EXPORT_PCA:\n",
    "        # save the pca transformed as well as the transformer\n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_r2l_u2r.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_test_r2l_u2r, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedWithPCA/x_test_l2_pca.txt', index=False)\n",
    "        \n",
    "    print(x.shape)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_train_r2l_u2r.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_train_r2l_u2r, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedWithPCA/x_train_l2_pca.txt', index=False)\n",
    "        \n",
    "    print(x.shape)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_validate_r2l_u2r.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_validate_r2l_u2r, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedWithPCA/x_val_l2_pca.txt', index=False)\n",
    "        \n",
    "    print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1768,
   "outputs": [],
   "source": [
    "if EXPORT_MODELS:\n",
    "    with open('EvalResources/Models/HGBC/l1_classifier.pkl', \"wb\") as f:\n",
    "        pickle.dump(dos_probe_classifier, f)\n",
    "    with open('EvalResources/Models/HGBC/l2_classifier.pkl', \"wb\") as f:\n",
    "        pickle.dump(r2l_u2r_classifier, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:18:45.776669500Z",
     "start_time": "2024-02-27T20:18:45.697570400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export one hot encoders\n",
    "if EXPORT_ENCODERS:\n",
    "    joblib.dump(ohe, 'EvalResources/Encoders/ohe_l1.pkl')\n",
    "    joblib.dump(ohe2, 'EvalResources/Encoders/ohe_l2.pkl')\n",
    "    joblib.dump(scaler1, 'EvalResources/Encoders/Scaler_l1.pkl')\n",
    "    joblib.dump(scaler2, 'EvalResources/Encoders/Scaler_l2.pkl')\n",
    "    joblib.dump(pca_dos_probe, 'EvalResources/Encoders/pca_transformer_l1.pkl')\n",
    "    joblib.dump(pca_r2l_u2r, 'EvalResources/Encoders/pca_transformer_l2.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1625,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:57.781247100Z",
     "start_time": "2024-02-27T20:11:57.719583100Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test1 = copy.deepcopy(df_test_original)\n",
    "df_test2 = copy.deepcopy(df_test_original)\n",
    "\n",
    "y_test_real = np.array([0 if x=='normal' else 1 for x in df_test1['label']])\n",
    "\n",
    "np.save(\"EvalResources/Test/KDDTest+_targets\", y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "outputs": [
    {
     "data": {
      "text/plain": "(22544,)"
     },
     "execution_count": 1626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:57.784313Z",
     "start_time": "2024-02-27T20:11:57.734839700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X_test1 = df_test1.loc[:, feature_names_l1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "outputs": [],
   "source": [
    "X_test1 = df_test1[common_features_l1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:57.870382100Z",
     "start_time": "2024-02-27T20:11:57.738206700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1628,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:57.920353500Z",
     "start_time": "2024-02-27T20:11:57.748845300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape for layer 1:  (22544, 28)\n"
     ]
    }
   ],
   "source": [
    "df_minmax = scaler1.transform(X_test1)\n",
    "X_test1 = pd.DataFrame(df_minmax, columns=X_test1.columns)\n",
    "label_enc = ohe.transform(df_test1.iloc[:,1:4])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test1 = pd.concat([X_test1, df_enc], axis=1)\n",
    "\n",
    "X_test_layer1 = pca_dos_probe.transform(X_test1)\n",
    "print('Test set shape for layer 1: ', X_test_layer1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1629,
   "outputs": [],
   "source": [
    "#X_test1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:57.938449Z",
     "start_time": "2024-02-27T20:11:57.817640300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1630,
   "outputs": [],
   "source": [
    "X_test2 = df_test2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:57.941293500Z",
     "start_time": "2024-02-27T20:11:57.822213700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X_test2 = df_test2.loc[:, feature_names_l2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1631,
   "outputs": [],
   "source": [
    "X_test2 = df_test2[common_features_l2] "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:57.944918100Z",
     "start_time": "2024-02-27T20:11:57.829637600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:58.015766Z",
     "start_time": "2024-02-27T20:11:57.834932900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape for layer 1:  (22544, 28)\n",
      "Test set shape for layer 2:  (22544, 13)\n"
     ]
    }
   ],
   "source": [
    "df_minmax = scaler2.transform(X_test2)\n",
    "X_test2 = pd.DataFrame(df_minmax, columns=X_test2.columns)\n",
    "label_enc = ohe2.transform(df_test2.iloc[:,1:4])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test2 = pd.concat([X_test2, df_enc], axis=1)\n",
    "\n",
    "X_test_layer2 = pca_r2l_u2r.transform(X_test2)\n",
    "print('Test set shape for layer 1: ', X_test_layer1.shape)\n",
    "print('Test set shape for layer 2: ', X_test_layer2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "outputs": [],
   "source": [
    "#X_test_layer1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:58.048233200Z",
     "start_time": "2024-02-27T20:11:57.878577200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1634,
   "outputs": [],
   "source": [
    "#X_test_layer2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:58.069363Z",
     "start_time": "2024-02-27T20:11:57.884069Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:58.072546400Z",
     "start_time": "2024-02-27T20:11:57.888188400Z"
    }
   },
   "outputs": [],
   "source": [
    "# same classifiers obtained above\n",
    "classifier1 = dos_probe_classifier\n",
    "classifier2 = r2l_u2r_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test the other classifiers obtained in the evaluation phase\n",
    "#classifier1 = joblib.load(\"EvalResources/Models/l1_clf.pkl\")\n",
    "#classifier2 = joblib.load(\"EvalResources/Models/l2_clf.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1636,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:59.584257100Z",
     "start_time": "2024-02-27T20:11:57.897317700Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1636], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m start \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(X_test_layer2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]):\n\u001B[1;32m---> 10\u001B[0m     layer1 \u001B[38;5;241m=\u001B[39m \u001B[43mclassifier1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_layer1\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m layer1 \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m     12\u001B[0m         result\u001B[38;5;241m.\u001B[39mappend(layer1)\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:366\u001B[0m, in \u001B[0;36mVotingClassifier.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    364\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    365\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvoting \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msoft\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 366\u001B[0m     maj \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# 'hard' voting\u001B[39;00m\n\u001B[0;32m    369\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_predict(X)\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:407\u001B[0m, in \u001B[0;36mVotingClassifier.predict_proba\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    393\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Compute probabilities of possible outcomes for samples in X.\u001B[39;00m\n\u001B[0;32m    394\u001B[0m \n\u001B[0;32m    395\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    403\u001B[0m \u001B[38;5;124;03m    Weighted average probability for each class per sample.\u001B[39;00m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    405\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    406\u001B[0m avg \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39maverage(\n\u001B[1;32m--> 407\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_collect_probas\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_weights_not_none\n\u001B[0;32m    408\u001B[0m )\n\u001B[0;32m    409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m avg\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:382\u001B[0m, in \u001B[0;36mVotingClassifier._collect_probas\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_collect_probas\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    381\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 382\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(\u001B[43m[\u001B[49m\u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mclf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimators_\u001B[49m\u001B[43m]\u001B[49m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_voting.py:382\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_collect_probas\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m    381\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Collect results from clf.predict calls.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 382\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray([\u001B[43mclf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_proba\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m clf \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_])\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:1919\u001B[0m, in \u001B[0;36mHistGradientBoostingClassifier.predict_proba\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m   1906\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_proba\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[0;32m   1907\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Predict class probabilities for X.\u001B[39;00m\n\u001B[0;32m   1908\u001B[0m \n\u001B[0;32m   1909\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;124;03m        The class probabilities of the input samples.\u001B[39;00m\n\u001B[0;32m   1918\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1919\u001B[0m     raw_predictions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_predict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1920\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss\u001B[38;5;241m.\u001B[39mpredict_proba(raw_predictions)\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:1052\u001B[0m, in \u001B[0;36mBaseHistGradientBoosting._raw_predict\u001B[1;34m(self, X, n_threads)\u001B[0m\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;66;03m# We intentionally decouple the number of threads used at prediction\u001B[39;00m\n\u001B[0;32m   1049\u001B[0m \u001B[38;5;66;03m# time from the number of threads used at fit time because the model\u001B[39;00m\n\u001B[0;32m   1050\u001B[0m \u001B[38;5;66;03m# can be deployed on a different machine for prediction purposes.\u001B[39;00m\n\u001B[0;32m   1051\u001B[0m n_threads \u001B[38;5;241m=\u001B[39m _openmp_effective_n_threads(n_threads)\n\u001B[1;32m-> 1052\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict_iterations\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1053\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predictors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraw_predictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_binned\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_threads\u001B[49m\n\u001B[0;32m   1054\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1055\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m raw_predictions\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:1080\u001B[0m, in \u001B[0;36mBaseHistGradientBoosting._predict_iterations\u001B[1;34m(self, X, predictors, raw_predictions, is_binned, n_threads)\u001B[0m\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1074\u001B[0m     predict \u001B[38;5;241m=\u001B[39m partial(\n\u001B[0;32m   1075\u001B[0m         predictor\u001B[38;5;241m.\u001B[39mpredict,\n\u001B[0;32m   1076\u001B[0m         known_cat_bitsets\u001B[38;5;241m=\u001B[39mknown_cat_bitsets,\n\u001B[0;32m   1077\u001B[0m         f_idx_map\u001B[38;5;241m=\u001B[39mf_idx_map,\n\u001B[0;32m   1078\u001B[0m         n_threads\u001B[38;5;241m=\u001B[39mn_threads,\n\u001B[0;32m   1079\u001B[0m     )\n\u001B[1;32m-> 1080\u001B[0m raw_predictions[:, k] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\predictor.py:71\u001B[0m, in \u001B[0;36mTreePredictor.predict\u001B[1;34m(self, X, known_cat_bitsets, f_idx_map, n_threads)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Predict raw values for non-binned data.\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \n\u001B[0;32m     50\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;124;03m    The raw predicted values.\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     70\u001B[0m out \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mempty(X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mY_DTYPE)\n\u001B[1;32m---> 71\u001B[0m \u001B[43m_predict_from_raw_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_left_cat_bitsets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     75\u001B[0m \u001B[43m    \u001B[49m\u001B[43mknown_cat_bitsets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     76\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf_idx_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m    \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppressing the warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")\n",
    "\n",
    "result = []\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(X_test_layer2.shape[0]):\n",
    "    layer1 = classifier1.predict(X_test_layer1[i].reshape(1, -1))[0]\n",
    "    if layer1 == 1:\n",
    "        result.append(layer1)\n",
    "    else:\n",
    "        layer2 = classifier2.predict(X_test_layer2[i].reshape(1, -1))[0]\n",
    "        if layer2 == 1:\n",
    "            result.append(layer2)\n",
    "        else:\n",
    "            result.append(0)\n",
    "clf_time = datetime.now() - start\n",
    "\n",
    "result = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:59.585257500Z",
     "start_time": "2024-02-27T20:11:59.585257500Z"
    }
   },
   "outputs": [],
   "source": [
    "# the results may vary\n",
    "# C=0.1, gamma=0.01\n",
    "print('Results for the full pipeline:')\n",
    "print(confusion_matrix(y_test_real,result))\n",
    "print('Accuracy = ', accuracy_score(y_test_real,result))\n",
    "print('F1 Score = ', f1_score(y_test_real,result))\n",
    "print('Precision = ', precision_score(y_test_real,result))\n",
    "print('Recall = ', recall_score(y_test_real,result))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_test_real,result))\n",
    "print('Classification time = ', clf_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = []\n",
    "TP = FP = TN = FN = 0\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(X_test_layer2.shape[0]):\n",
    "    layer1 = classifier1.predict(X_test_layer1[i].reshape(1, -1))[0]\n",
    "    if layer1 == 1:\n",
    "        result.append(layer1)\n",
    "        if y_test_real[i] == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    else:\n",
    "        layer2 = classifier2.predict(X_test_layer2[i].reshape(1, -1))[0]\n",
    "        if layer2 == 1:\n",
    "            result.append(layer2)\n",
    "            if y_test_real[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else:\n",
    "            result.append(0)\n",
    "            if y_test_real[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "clf_time = datetime.now() - start\n",
    "\n",
    "result = np.array(result)\n",
    "\n",
    "# Compute TPR, FPR, TNR, FNR\n",
    "TPR = TP / (TP + FN)\n",
    "FPR = FP / (FP + TN)\n",
    "TNR = TN / (TN + FP)\n",
    "FNR = FN / (FN + TP)\n",
    "\n",
    "# Print TPR, FPR, TNR, FNR\n",
    "print('True Positive Rate (TPR) =', TPR)\n",
    "print('False Positive Rate (FPR) =', FPR)\n",
    "print('True Negative Rate (TNR) =', TNR)\n",
    "print('False Negative Rate (FNR) =', FNR)\n",
    "print('Classification time = ', clf_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.586256400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ROC graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_real, result)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig('static_roc_curve-hgbc.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T20:11:59.663378900Z",
     "start_time": "2024-02-27T20:11:59.588487500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "if True:\n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_layer1.shape[1] + 1)]\n",
    "    X1_test = pd.DataFrame(data=X_test_layer1, columns=column_names)\n",
    "    X1_test.to_csv('EvalResources/AdditionalSets/x_test_l1_pca.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_layer2.shape[1] + 1)]\n",
    "    X2_test = pd.DataFrame(data=X_test_layer2, columns=column_names)\n",
    "    X2_test.to_csv('EvalResources/AdditionalSets/x_test_l2_pca.txt', index=False)\n",
    "    \n",
    "    np.save('EvalResources/AdditionalSets/y_test', y_test_real)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate seen and unseen attack categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.589690Z"
    }
   },
   "outputs": [],
   "source": [
    "# load testset\n",
    "df_test = pd.read_csv('EvalResources/KDDTest+.txt', sep=\",\", header=None, skipinitialspace = True)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "y_test = df_test['label']\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "\n",
    "df_test_original = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if EXPORT_DATASETS:\n",
    "    df_test_original.to_csv('EvalResources/ProcessedDatasets/x_test_full.txt', index=False)\n",
    "    np.save('EvalResources/ProcessedDatasets/y_test_full', y_test)\n",
    "    \n",
    "#df_test_original"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.591978800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.593009900Z"
    }
   },
   "outputs": [],
   "source": [
    "new_attack = []\n",
    "for i in df_test_original['label'].value_counts().index.tolist()[1:]:\n",
    "    if i not in df_train_original['label'].value_counts().index.tolist()[1:]:\n",
    "        new_attack.append(i)\n",
    "        \n",
    "new_attack.sort()\n",
    "#new_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.595009900Z"
    }
   },
   "outputs": [],
   "source": [
    "index_of_new_attacks = []\n",
    "\n",
    "for i in range(len(df_test_original)):\n",
    "    if df_test_original['label'][i] in new_attack:\n",
    "        index_of_new_attacks.append(df_test_original.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#len(index_of_new_attacks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.597853700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.601570100Z"
    }
   },
   "outputs": [],
   "source": [
    "new_attack.append('normal')\n",
    "#new_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.606331900Z"
    }
   },
   "outputs": [],
   "source": [
    "index_of_old_attacks = []\n",
    "\n",
    "for i in range(len(df_test_original)):\n",
    "    if df_test_original['label'][i] not in new_attack:\n",
    "        index_of_old_attacks.append(df_test_original.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.609969500Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of new attacks in the test set: ', result[index_of_new_attacks].shape[0])\n",
    "print('Number of new attacks detected by the classifiers: ', result[index_of_new_attacks].sum())\n",
    "print('Proportion of new attacks detected: ', result[index_of_new_attacks].sum()/result[index_of_new_attacks].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.611753600Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of old attacks in the test set: ', result[index_of_old_attacks].shape[0])\n",
    "print('Number of old attacks detected by the classifiers: ', result[index_of_old_attacks].sum())\n",
    "print('Proportion of old attacks detected: ', result[index_of_old_attacks].sum()/result[index_of_old_attacks].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate single attack types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-27T20:11:59.612781300Z"
    }
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "df_test = pd.read_csv('EvalResources/KDDTest+.txt', sep=\",\", header=None, skipinitialspace = True)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "y_test = df_test['label']\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "df_test_original = df_test\n",
    "df = df_test_original\n",
    "\n",
    "dos_index = df.index[(df['label'].isin(dos_attacks))].tolist()\n",
    "probe_index = df.index[(df['label'].isin(probe_attacks))].tolist()\n",
    "r2l_index = df.index[(df['label'].isin(r2l_attacks))].tolist()\n",
    "u2r_index = df.index[(df['label'].isin(u2r_attacks))].tolist()\n",
    "\n",
    "print('Evaluation split into single attack type:')\n",
    "print(\"Number of dos attacks: \", result[dos_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[dos_index].sum())\n",
    "print(\"Ratio of detection: \", result[dos_index].sum()/result[dos_index].shape[0])\n",
    "\n",
    "print(\"Number of probe attacks: \", result[probe_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[probe_index].sum())\n",
    "print(\"Ratio of detection: \", result[probe_index].sum()/result[probe_index].shape[0])\n",
    "\n",
    "print(\"Number of r2l attacks: \", result[r2l_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[r2l_index].sum())\n",
    "print(\"Ratio of detection: \", result[r2l_index].sum()/result[r2l_index].shape[0])\n",
    "\n",
    "print(\"Number of u2r attacks: \", result[u2r_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[u2r_index].sum())\n",
    "print(\"Ratio of detection: \", result[u2r_index].sum()/result[u2r_index].shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
