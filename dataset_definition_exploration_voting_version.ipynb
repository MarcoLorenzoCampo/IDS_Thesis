{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:32.480540300Z",
     "start_time": "2024-03-05T16:00:31.483585800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import RandomUnderSampler as under_sam\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.052753100Z",
     "start_time": "2024-03-05T16:00:32.482728200Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the train set\n",
    "df_train = pd.read_csv('EvalResources/KDDTrain+.txt', sep=\",\", header=None, skipinitialspace = True)\n",
    "df_train = df_train[df_train.columns[:-1]]  # tags column\n",
    "titles = pd.read_csv('EvalResources/Field Names.csv', header=None, skipinitialspace = True)\n",
    "label = pd.Series(['label'], index=[41])\n",
    "titles = pd.concat([titles[0], label])\n",
    "df_train.columns = titles.to_list()\n",
    "df_train = df_train.drop(['num_outbound_cmds'],axis=1)\n",
    "df_train_original = df_train\n",
    "\n",
    "# df_train_original.to_csv('KB Process/NSL-KDD Original Datasets/KDDTrain+_with_labels.txt', index=False)\n",
    "\n",
    "#df_train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.138866500Z",
     "start_time": "2024-03-05T16:00:33.020012700Z"
    }
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "df_test = pd.read_csv('EvalResources/KDDTest+.txt', sep=\",\", header=None, skipinitialspace = True)\n",
    "df_test_ = df_test.sort_index(axis=1)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "df_test_original = df_test\n",
    "\n",
    "# df_test_original.to_csv('KB Process/NSL-KDD Original Datasets/KDDTest+.txt', index=False)\n",
    "\n",
    "#df_test_original"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Execution Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [],
   "source": [
    "EXPORT_MODELS = 0\n",
    "EXPORT_DATASETS = 0\n",
    "EXPORT_PCA = 0\n",
    "EXPORT_ENCODERS = 0\n",
    "\n",
    "pd.options.display.max_columns = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.142672700Z",
     "start_time": "2024-03-05T16:00:33.094009600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "# list of single attacks \n",
    "dos_attacks = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop', 'worm', 'apache2', 'mailbomb', 'processtable', 'udpstorm']\n",
    "probe_attacks = ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']\n",
    "r2l_attacks = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster',\n",
    "                'snmpguess', 'spy', 'warezclient', 'httptunnel', 'named', 'sendmail', 'snmpgetattack', 'xlock', 'xsnoop']\n",
    "u2r_attacks = ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm'] \n",
    "\n",
    "# list of attack classes split according to detection layer\n",
    "dos_probe_list = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop', 'ipsweep', 'nmap', 'portsweep', 'satan']\n",
    "dos_probe_test = ['apache2', 'mailbomb', 'processtable', 'udpstorm', 'mscan', 'saint']\n",
    "u2r_r2l_list = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster',\n",
    "                'snmpguess', 'spy', 'warezclient', 'buffer_overflow', 'loadmodule', 'rootkit', 'perl']\n",
    "u2r_r2l_test = ['httptunnel', 'named', 'sendmail', 'snmpgetattack', 'xlock', 'xsnoop', 'ps', 'xterm', 'sqlattack']\n",
    "normal_list = ['normal']\n",
    "categorical_features = ['protocol_type', 'service', 'flag']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.154617Z",
     "start_time": "2024-03-05T16:00:33.099536300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "outputs": [],
   "source": [
    "# load the features obtained with ICFS for both layer 1 and layer 2\n",
    "with open('KBProcess/AWS Downloads/MinimalFeatures/NSL_features_l1.txt', 'r') as f:\n",
    "    common_features_l1 = f.read().split(',')\n",
    "\n",
    "with open('KBProcess/AWS Downloads/MinimalFeatures/NSL_features_l2.txt', 'r') as f:\n",
    "    common_features_l2 = f.read().split(',')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.160193700Z",
     "start_time": "2024-03-05T16:00:33.105658400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [],
   "source": [
    "df_train_and_validate = copy.deepcopy(df_train_original)\n",
    "df_test = copy.deepcopy(df_test_original)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.213260Z",
     "start_time": "2024-03-05T16:00:33.109686400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [],
   "source": [
    "# Save all the targets for the dataset\n",
    "\n",
    "y_test_l1 = [1 if x in (dos_attacks+probe_attacks) else 0 for x in df_test['label']]\n",
    "y_test_l2 = [1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_test['label']]\n",
    "\n",
    "if EXPORT_DATASETS:\n",
    "    np.save(\"EvalResources/AdditionalSets/l1_full_test_targets.npy\", y_test_l1)\n",
    "    np.save(\"EvalResources/AdditionalSets/l2_full_test_targets.npy\", y_test_l2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.257811500Z",
     "start_time": "2024-03-05T16:00:33.127186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [],
   "source": [
    "# add an additional column to th dataframe to perform splitting\n",
    "\n",
    "attacks = ['dos' if x in dos_attacks else\n",
    "           'probe' if x in probe_attacks else\n",
    "           'u2r' if x in u2r_attacks else\n",
    "           'r2l' if x in r2l_attacks else\n",
    "           'normal' for x in df_train_and_validate['label']]\n",
    "\n",
    "# add the column to the dataframe\n",
    "df_train_and_validate['attacks'] = attacks"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.297409900Z",
     "start_time": "2024-03-05T16:00:33.141667400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOS PROBE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.416211300Z",
     "start_time": "2024-03-05T16:00:33.179471400Z"
    }
   },
   "outputs": [],
   "source": [
    "# split in test and validation set for BOTH layers\n",
    "df_train_original, df_val_original = train_test_split(df_train_and_validate, test_size=0.2, stratify=df_train_and_validate['attacks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and 'column_name' is the specific column you want to plot\n",
    "plt.hist(df_val_original['attacks'], bins=10)  # Adjust the number of bins as needed\n",
    "plt.xlabel('Values')  # Set x-axis label\n",
    "plt.ylabel('Frequency')  # Set y-axis label\n",
    "plt.title('Histogram of Column')  # Set title\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and 'column_name' is the specific column you want to plot\n",
    "plt.hist(df_train_original['attacks'], bins=10)  # Adjust the number of bins as needed\n",
    "plt.xlabel('Values')  # Set x-axis label\n",
    "plt.ylabel('Frequency')  # Set y-axis label\n",
    "plt.title('Histogram of Column')  # Set title\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "# dataframes specifically for layer 1\n",
    "df_train = copy.deepcopy(df_train_original)\n",
    "df_val = copy.deepcopy(df_val_original)\n",
    "df_test = copy.deepcopy(df_test_original)\n",
    "\n",
    "# target variables for all layers\n",
    "y_train_full = np.array([1 if x not in normal_list else 0 for x in df_train['label']])\n",
    "y_test_full = np.array([1 if x not in normal_list else 0 for x in df_test['label']])\n",
    "\n",
    "# set the target variables accordingly\n",
    "y_train_l1 = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_train['label']])\n",
    "y_validate_l1 = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_val['label']])\n",
    "y_test = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_test ['label']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.435357200Z",
     "start_time": "2024-03-05T16:00:33.302567900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [
    "# this dataframe contains the whole train set \n",
    "df_train = df_train.drop(['label'],axis=1)\n",
    "df_train = df_train.reset_index().drop(['index'], axis=1)\n",
    "#df_train"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.495124400Z",
     "start_time": "2024-03-05T16:00:33.386123700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "# this dataframe contains the whole validation set\n",
    "df_val = df_val.drop(['label'],axis=1)\n",
    "df_val = df_val.reset_index().drop(['index'], axis=1)\n",
    "#df_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.500859500Z",
     "start_time": "2024-03-05T16:00:33.428458500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "# this dataframe contains the whole test set\n",
    "df_test = df_test.drop(['label'],axis=1)\n",
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "#df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.522568200Z",
     "start_time": "2024-03-05T16:00:33.457613600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using features obtained with a random forest on numerical features only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# List of feature names\n",
    "feature_names_l1 = ['src_bytes', 'logged_in', 'count', 'srv_count', 'srv_serror_rate',\n",
    "       'same_srv_rate', 'diff_srv_rate', 'dst_host_srv_count',\n",
    "       'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
    "       'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "       'dst_host_srv_rerror_rate']\n",
    "\n",
    "# Selecting features using loc\n",
    "X_train = df_train.loc[:, feature_names_l1]\n",
    "X_validate = df_val.loc[:, feature_names_l1]\n",
    "X_test = df_test.loc[:, feature_names_l1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "# Using the features extracted by using the ICFS algorithm\n",
    "X_train = df_train[common_features_l1]\n",
    "X_validate = df_val[common_features_l1]\n",
    "X_test = df_test[common_features_l1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.609716200Z",
     "start_time": "2024-03-05T16:00:33.523789300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "# 2 one-hot encoders, one for the features of layer1 and one for the features of layer2\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe2 = OneHotEncoder(handle_unknown='ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.670990200Z",
     "start_time": "2024-03-05T16:00:33.591941800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.718736600Z",
     "start_time": "2024-03-05T16:00:33.660376100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.759573600Z",
     "start_time": "2024-03-05T16:00:33.708731900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "# scaling the train set for layer1\n",
    "df_minmax = scaler1.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(df_minmax, columns=X_train.columns)\n",
    "\n",
    "#X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.825508500Z",
     "start_time": "2024-03-05T16:00:33.759573600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [],
   "source": [
    "# scaling the validation set for layer1\n",
    "df_minmax_val = scaler1.transform(X_validate)\n",
    "X_validate = pd.DataFrame(df_minmax_val, columns=X_validate.columns)\n",
    "\n",
    "#X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.850139Z",
     "start_time": "2024-03-05T16:00:33.803232200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "# scaling the test set for layer1\n",
    "df_minmax_test = scaler1.transform(X_test)\n",
    "X_test = pd.DataFrame(df_minmax_test, columns=X_test.columns)\n",
    "\n",
    "#X_vtest"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:33.931772Z",
     "start_time": "2024-03-05T16:00:33.850139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the train set\n",
    "label_enc = ohe.fit_transform(df_train[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_train = pd.concat([X_train, df_enc], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:34.087012600Z",
     "start_time": "2024-03-05T16:00:33.925477300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the validation set\n",
    "label_enc = ohe.transform(df_val[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_validate = pd.concat([X_validate, df_enc], axis=1)\n",
    "\n",
    "#X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:34.142417300Z",
     "start_time": "2024-03-05T16:00:34.045543900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the test set\n",
    "label_enc = ohe.transform(df_test[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test = pd.concat([X_test, df_enc], axis=1)\n",
    "\n",
    "#X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:34.184884200Z",
     "start_time": "2024-03-05T16:00:34.088951Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:34.195734100Z",
     "start_time": "2024-03-05T16:00:34.118138500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the whole train set:  (100778, 100)\n",
      "Shape of its targets:  (100778,)\n",
      "Shape of the whole train set:  (25195, 100)\n",
      "Shape of its targets:  (25195,)\n",
      "Shape of the whole test set:  (22544, 100)\n",
      "Shape of its targets:  (22544,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the whole train set: ', X_train.shape)\n",
    "print('Shape of its targets: ', y_train_l1.shape)\n",
    "print('Shape of the whole train set: ', X_validate.shape)\n",
    "print('Shape of its targets: ', y_validate_l1.shape)\n",
    "print('Shape of the whole test set: ', X_test.shape)\n",
    "print('Shape of its targets: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export the dataset for training layer 1\n",
    "if EXPORT_DATASETS:\n",
    "    X_train.to_csv('EvalResources/ProcessedDatasets/x_train_l1.txt', index=False)\n",
    "    X_validate.to_csv('EvalResources/ProcessedDatasets/x_val_l1.txt', index=False)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_train_l1', y_train_l1)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_val_l1', y_validate_l1)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_test_l1', y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Principal Component Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "outputs": [
    {
     "data": {
      "text/plain": "(100778, 28)"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_dos_probe = PCA(n_components=0.95)\n",
    "X_train_dos_probe = pca_dos_probe.fit_transform(X_train)\n",
    "X_test_dos_probe = pca_dos_probe.transform(X_test)\n",
    "X_validate_dos_probe = pca_dos_probe.transform(X_validate)\n",
    "\n",
    "# X_train = X_train.sort_index(axis=1)\n",
    "X_train_dos_probe.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:35.728556300Z",
     "start_time": "2024-03-05T16:00:34.121654900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [],
   "source": [
    "if EXPORT_PCA:\n",
    "    # save the pca transformed datasets for layer1\n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_dos_probe.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_test_dos_probe, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedDatasets/KDDTest+_l1_pca.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_train_dos_probe.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_train_dos_probe, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedDatasets/KDDTrain+_l1_pca.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_validate_dos_probe.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_validate_dos_probe, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedDatasets/KDDValidate+_l1_pca.txt', index=False)\n",
    "    \n",
    "    # save the correspondant target values\n",
    "    np.save(\"EvalResources/ProcessedDatasets//KDDTrain+_l1_targets\", y_train_l1)\n",
    "    np.save(\"EvalResources/ProcessedDatasets/KDDValidate+_l1_targets\", y_validate_l1)\n",
    "    np.save(\"EvalResources/ProcessedDatasets//KDDTest+_l1_targets\", y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:35.763909400Z",
     "start_time": "2024-03-05T16:00:35.728048Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building the classifier for the layer1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:35.767909500Z",
     "start_time": "2024-03-05T16:00:35.732048800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "# Voting classifiers\n",
    "\n",
    "voting_classifiers = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:00:35.784955100Z",
     "start_time": "2024-03-05T16:00:35.736301400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Using HistGradientBoosting classifier\n",
    "dos_probe_classifier = HistGradientBoostingClassifier()\n",
    "\n",
    "start = datetime.now()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "\n",
    "voting_classifiers.append((\"hgbc\", dos_probe_classifier))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "# Using Random Forest Classifier\n",
    "dos_probe_classifier = RandomForestClassifier()\n",
    "\n",
    "start = datetime.now()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "\n",
    "voting_classifiers.append((\"rf\", dos_probe_classifier))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:01:42.054979300Z",
     "start_time": "2024-03-05T16:00:35.739211200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for layer 1:\n",
      "Confusion matrix: [TP FN / FP TN]\n",
      " [[12183   480]\n",
      " [ 2361  7520]]\n",
      "Accuracy =  0.8739797728885734\n",
      "F1 Score =  0.8411162686650635\n",
      "Precision =  0.94\n",
      "Recall =  0.7610565732213339\n",
      "Train time =  0:01:06.156840\n"
     ]
    }
   ],
   "source": [
    "print('Metrics for layer 1:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Train time = ', ttime)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:01:42.105530800Z",
     "start_time": "2024-03-05T16:01:42.058155600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# test the other classifiers obtained in the evaluation phase\n",
    "dos_probe_classifier = joblib.load(\"TunerProcess/TunedModels/l1_classifier.pkl\")\n",
    "#classifier2 = joblib.load(\"TunerProcess/TunedModels/l1_classifier.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for layer 1:\n",
      "Confusion matrix: [TP FN / FP TN]\n",
      " [[9809 2854]\n",
      " [ 974 8907]]\n",
      "Accuracy =  0.8301987224982257\n",
      "F1 Score =  0.8231217077904076\n",
      "Precision =  0.7573335600714225\n",
      "Recall =  0.90142698107479\n",
      "Train time =  0:00:00.060568\n",
      "Shape of the train set for l1:  (100778, 28)\n"
     ]
    }
   ],
   "source": [
    "# Using the Naive Bayes Classifier\n",
    "dos_probe_classifier = GaussianNB()\n",
    "\n",
    "start = datetime.now()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "\n",
    "print('Metrics for layer 1:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "print('Shape of the train set for l1: ', X_train_dos_probe.shape)\n",
    "\n",
    "voting_classifiers.append((\"nbc\", dos_probe_classifier))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:01:42.213887800Z",
     "start_time": "2024-03-05T16:01:42.083674100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Voting classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "dos_probe_classifier = VotingClassifier(estimators=voting_classifiers, voting='soft')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:01:42.217444200Z",
     "start_time": "2024-03-05T16:01:42.180431900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a voting classifier:\n",
      "Confusion matrix: [TP FN / FP TN]\n",
      " [[11242  1421]\n",
      " [  832  9049]]\n",
      "Accuracy =  0.9000621007806955\n",
      "F1 Score =  0.8892929094393395\n",
      "Precision =  0.8642788920725883\n",
      "Recall =  0.9157979961542354\n",
      "Train time =  0:01:08.987743\n",
      "Shape of the train set for l1:  (100778, 28)\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "ttime = datetime.now() - start\n",
    "\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "\n",
    "print('Using a voting classifier:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "print('Shape of the train set for l1: ', X_train_dos_probe.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.398746600Z",
     "start_time": "2024-03-05T16:01:42.183922Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2L+U2R classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.488850300Z",
     "start_time": "2024-03-05T16:02:51.394650400Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = copy.deepcopy(df_train_original)\n",
    "df_test = copy.deepcopy(df_test_original)\n",
    "df_val = copy.deepcopy(df_val_original)\n",
    "\n",
    "# load targeted attacks (Normal + r2l + u2r)\n",
    "df_train = df_train[df_train['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "df_val = df_val[df_val['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "df_test = df_test[df_test['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "\n",
    "# set the target variables accordingly\n",
    "y_train_l2 = np.array([1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_train['label']])\n",
    "y_validate_l2 = np.array([1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_val['label']])\n",
    "y_test = np.array([1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_test['label']])\n",
    "\n",
    "df_train = df_train.drop(['label'],axis=1)\n",
    "df_train = df_train.reset_index().drop(['index'], axis=1)\n",
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [],
   "source": [
    "df_val = df_val.drop(['label'],axis=1)\n",
    "df_val = df_val.reset_index().drop(['index'], axis=1)\n",
    "#df_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.517489100Z",
     "start_time": "2024-03-05T16:02:51.488850300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['label'],axis=1)\n",
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "#df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.534572Z",
     "start_time": "2024-03-05T16:02:51.500253900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "X_train = df_train\n",
    "X_validate = df_val\n",
    "X_test = df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.574730400Z",
     "start_time": "2024-03-05T16:02:51.511340Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using features obtained with a random forest on numerical features only"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# List of feature names\n",
    "feature_names_l2 = ['count', 'is_guest_login', 'srv_count', 'hot', 'dst_host_serror_rate',\n",
    "       'dst_host_srv_count', 'dst_host_count', 'dst_host_same_srv_rate',\n",
    "       'dst_host_same_src_port_rate', 'num_file_creations', 'diff_srv_rate']\n",
    "\n",
    "# Selecting features using loc\n",
    "X_train = df_train.loc[:, feature_names_l2]\n",
    "X_validate = df_val.loc[:, feature_names_l2]\n",
    "X_test = df_test.loc[:, feature_names_l2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "X_train = df_train[common_features_l2]\n",
    "X_validate = df_val[common_features_l2]\n",
    "X_test = df_test[common_features_l2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.594410Z",
     "start_time": "2024-03-05T16:02:51.529491400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.625387800Z",
     "start_time": "2024-03-05T16:02:51.533409800Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax = scaler2.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(df_minmax, columns=X_train.columns)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "df_minmax = scaler2.transform(X_validate)\n",
    "X_validate = pd.DataFrame(df_minmax, columns=X_validate.columns)\n",
    "#X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.644956100Z",
     "start_time": "2024-03-05T16:02:51.549912500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [],
   "source": [
    "df_minmax = scaler2.transform(X_test)\n",
    "X_test = pd.DataFrame(df_minmax, columns=X_test.columns)\n",
    "#X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.650955900Z",
     "start_time": "2024-03-05T16:02:51.557991900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.766792500Z",
     "start_time": "2024-03-05T16:02:51.566172600Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the train set\n",
    "label_enc = ohe2.fit_transform(df_train[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_train = pd.concat([X_train, df_enc], axis=1)\n",
    "#X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the validation set\n",
    "label_enc = ohe2.transform(df_val[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_validate = pd.concat([X_validate, df_enc], axis=1)\n",
    "#X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.775303500Z",
     "start_time": "2024-03-05T16:02:51.625925500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the test set\n",
    "label_enc = ohe2.transform(df_test[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test = pd.concat([X_test, df_enc], axis=1)\n",
    "#X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.780816600Z",
     "start_time": "2024-03-05T16:02:51.647957400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.804038600Z",
     "start_time": "2024-03-05T16:02:51.671698600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train set:  (54712, 51)\n",
      "Shape of its target:  (54712,)\n",
      "Shape of the test set:  (12663, 51)\n",
      "Shape of its target:  (12663,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the train set: ', X_train.shape)\n",
    "print('Shape of its target: ', y_train_l2.shape)\n",
    "print('Shape of the test set: ', X_test.shape)\n",
    "print('Shape of its target: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "# Under sampling the train set for l2\n",
    "sm = under_sam(sampling_strategy=1)\n",
    "X_train, y_train_l2 = sm.fit_resample(X_train,y_train_l2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.878637500Z",
     "start_time": "2024-03-05T16:02:51.676185300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export the datasets\n",
    "Train set has been scaled, one hot encoded, undersampled\n",
    "Test set has been scaled and one hot encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export the dataset for training layer 2\n",
    "if EXPORT_DATASETS:\n",
    "    # X_train.to_csv('EvalResources/ProcessedDatasets/x_train_l2.txt', index=False)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_train_l2', y_train_l2)\n",
    "    # X_validate.to_csv('EvalResources/ProcessedDatasets/x_val_l2.txt', index=False)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_val_l2', y_val)\n",
    "    np.save('EvalResources/ProcessedWithPCA/y_test_l2', y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "pca_r2l_u2r = PCA(n_components=0.95)\n",
    "X_train_r2l_u2r = pca_r2l_u2r.fit_transform(X_train)\n",
    "X_test_r2l_u2r = pca_r2l_u2r.transform(X_test)\n",
    "X_validate_r2l_u2r = pca_r2l_u2r.transform(X_validate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.910456300Z",
     "start_time": "2024-03-05T16:02:51.700733800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "# Support Vector Machine for layer l2\n",
    "r2l_u2r_classifier = SVC(probability=True)\n",
    "\n",
    "#r2l_u2r_classifier = SVC()\n",
    "\n",
    "start = datetime.now()\n",
    "r2l_u2r_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "ttime = datetime.now() - start"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:51.992669100Z",
     "start_time": "2024-03-05T16:02:51.734924200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "predicted = r2l_u2r_classifier.predict(X_test_r2l_u2r)\n",
    "\n",
    "print('Metrics for layer 2:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_test,predicted))\n",
    "print('Train time = ', ttime)\n",
    "print('Shape of the training set: ', X_train_r2l_u2r.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "predicted = r2l_u2r_classifier.predict(X_train_r2l_u2r)\n",
    "\n",
    "print('Metrics for layer 2:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_train_l2,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_train_l2,predicted))\n",
    "print('F1 Score = ', f1_score(y_train_l2,predicted))\n",
    "print('Precision = ', precision_score(y_train_l2,predicted))\n",
    "print('Recall = ', recall_score(y_train_l2,predicted))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_train_l2,predicted))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [],
   "source": [
    "if EXPORT_PCA:\n",
    "    # save the pca transformed as well as the transformer\n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_r2l_u2r.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_test_r2l_u2r, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedDatasets/KDDTest+_l2_pca.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_train_r2l_u2r.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_train_r2l_u2r, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedDatasets/KDDTrain+_l2_pca.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_validate_r2l_u2r.shape[1] + 1)]\n",
    "    x = pd.DataFrame(data=X_validate_r2l_u2r, columns=column_names)\n",
    "    x.to_csv('EvalResources/ProcessedDatasets/KDDValidate+_l2_pca.txt', index=False)\n",
    "\n",
    "    # export the correspondant targets values\n",
    "    np.save(\"EvalResources/ProcessedDatasets//KDDTrain+_l2_targets\", y_train_l2)\n",
    "    np.save(\"EvalResources/ProcessedDatasets/KDDValidate+_l2_targets\", y_validate_l2)\n",
    "    np.save(\"EvalResources/ProcessedDatasets//KDDTest+_l2_targets\", y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.009631600Z",
     "start_time": "2024-03-05T16:02:51.822661700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "if EXPORT_MODELS:\n",
    "    with open('EvalResources/Models/HGBC/l1_classifier.pkl', \"wb\") as f:\n",
    "        pickle.dump(dos_probe_classifier, f)\n",
    "    with open('EvalResources/Models/HGBC/l2_classifier.pkl', \"wb\") as f:\n",
    "        pickle.dump(r2l_u2r_classifier, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.012631500Z",
     "start_time": "2024-03-05T16:02:51.825669600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export one hot encoders\n",
    "if EXPORT_ENCODERS:\n",
    "    joblib.dump(ohe, 'EvalResources/Encoders/ohe_l1.pkl')\n",
    "    joblib.dump(ohe2, 'EvalResources/Encoders/ohe_l2.pkl')\n",
    "    joblib.dump(scaler1, 'EvalResources/Encoders/Scaler_l1.pkl')\n",
    "    joblib.dump(scaler2, 'EvalResources/Encoders/Scaler_l2.pkl')\n",
    "    joblib.dump(pca_dos_probe, 'EvalResources/Encoders/pca_transformer_l1.pkl')\n",
    "    joblib.dump(pca_r2l_u2r, 'EvalResources/Encoders/pca_transformer_l2.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.127995800Z",
     "start_time": "2024-03-05T16:02:51.830996200Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test1 = copy.deepcopy(df_test_original)\n",
    "df_test2 = copy.deepcopy(df_test_original)\n",
    "\n",
    "y_test_real = np.array([0 if x=='normal' else 1 for x in df_test1['label']])\n",
    "\n",
    "np.save(\"EvalResources/Test/KDDTest+_targets\", y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "data": {
      "text/plain": "(22544,)"
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.130506500Z",
     "start_time": "2024-03-05T16:02:51.846728500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X_test1 = df_test1.loc[:, feature_names_l1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "outputs": [],
   "source": [
    "X_test1 = df_test1[common_features_l1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.148428100Z",
     "start_time": "2024-03-05T16:02:51.852167400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.168540600Z",
     "start_time": "2024-03-05T16:02:51.861514300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape for layer 1:  (22544, 28)\n"
     ]
    }
   ],
   "source": [
    "df_minmax = scaler1.transform(X_test1)\n",
    "X_test1 = pd.DataFrame(df_minmax, columns=X_test1.columns)\n",
    "label_enc = ohe.transform(df_test1.iloc[:,1:4])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test1 = pd.concat([X_test1, df_enc], axis=1)\n",
    "\n",
    "X_test_layer1 = pca_dos_probe.transform(X_test1)\n",
    "print('Test set shape for layer 1: ', X_test_layer1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "#X_test1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.171567100Z",
     "start_time": "2024-03-05T16:02:51.919542900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [],
   "source": [
    "X_test2 = df_test2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.181984100Z",
     "start_time": "2024-03-05T16:02:51.922565900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "X_test2 = df_test2.loc[:, feature_names_l2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "X_test2 = df_test2[common_features_l2] "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.184983900Z",
     "start_time": "2024-03-05T16:02:51.929335700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.202161500Z",
     "start_time": "2024-03-05T16:02:51.936853100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape for layer 1:  (22544, 28)\n",
      "Test set shape for layer 2:  (22544, 12)\n"
     ]
    }
   ],
   "source": [
    "df_minmax = scaler2.transform(X_test2)\n",
    "X_test2 = pd.DataFrame(df_minmax, columns=X_test2.columns)\n",
    "label_enc = ohe2.transform(df_test2.iloc[:,1:4])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test2 = pd.concat([X_test2, df_enc], axis=1)\n",
    "\n",
    "X_test_layer2 = pca_r2l_u2r.transform(X_test2)\n",
    "print('Test set shape for layer 1: ', X_test_layer1.shape)\n",
    "print('Test set shape for layer 2: ', X_test_layer2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "#X_test_layer1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.208252800Z",
     "start_time": "2024-03-05T16:02:51.984110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [],
   "source": [
    "#X_test_layer2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.213759800Z",
     "start_time": "2024-03-05T16:02:51.989785100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:02:52.230179800Z",
     "start_time": "2024-03-05T16:02:51.996566200Z"
    }
   },
   "outputs": [],
   "source": [
    "# same classifiers obtained above\n",
    "classifier1 = dos_probe_classifier\n",
    "classifier2 = r2l_u2r_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:04:29.486103100Z",
     "start_time": "2024-03-05T16:02:51.998626800Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppressing the warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")\n",
    "\n",
    "result = []\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(X_test_layer2.shape[0]):\n",
    "    layer1 = classifier1.predict(X_test_layer1[i].reshape(1, -1))[0]\n",
    "    if layer1 == 1:\n",
    "        result.append(layer1)\n",
    "    else:\n",
    "        layer2 = classifier2.predict(X_test_layer2[i].reshape(1, -1))[0]\n",
    "        if layer2 == 1:\n",
    "            result.append(layer2)\n",
    "        else:\n",
    "            result.append(0)\n",
    "clf_time = datetime.now() - start\n",
    "\n",
    "result = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:04:29.533729700Z",
     "start_time": "2024-03-05T16:04:29.486103100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the full pipeline:\n",
      "[[ 8779   932]\n",
      " [ 2114 10719]]\n",
      "Accuracy =  0.8648864442867282\n",
      "F1 Score =  0.8755922234928933\n",
      "Precision =  0.9200068663634022\n",
      "Recall =  0.8352684485311307\n",
      "Matthew corr =  0.7325856018655161\n",
      "Classification time =  0:01:37.479352\n"
     ]
    }
   ],
   "source": [
    "# the results may vary\n",
    "# C=0.1, gamma=0.01\n",
    "print('Results for the full pipeline:')\n",
    "print(confusion_matrix(y_test_real,result))\n",
    "print('Accuracy = ', accuracy_score(y_test_real,result))\n",
    "print('F1 Score = ', f1_score(y_test_real,result))\n",
    "print('Precision = ', precision_score(y_test_real,result))\n",
    "print('Recall = ', recall_score(y_test_real,result))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_test_real,result))\n",
    "print('Classification time = ', clf_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate (TPR) = 0.8352684485311307\n",
      "False Positive Rate (FPR) = 0.09597363814231284\n",
      "True Negative Rate (TNR) = 0.9040263618576871\n",
      "False Negative Rate (FNR) = 0.1647315514688693\n",
      "Classification time =  0:01:39.645901\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "TP = FP = TN = FN = 0\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(X_test_layer2.shape[0]):\n",
    "    layer1 = classifier1.predict(X_test_layer1[i].reshape(1, -1))[0]\n",
    "    if layer1 == 1:\n",
    "        result.append(layer1)\n",
    "        if y_test_real[i] == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    else:\n",
    "        layer2 = classifier2.predict(X_test_layer2[i].reshape(1, -1))[0]\n",
    "        if layer2 == 1:\n",
    "            result.append(layer2)\n",
    "            if y_test_real[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else:\n",
    "            result.append(0)\n",
    "            if y_test_real[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "clf_time = datetime.now() - start\n",
    "\n",
    "result = np.array(result)\n",
    "\n",
    "# Compute TPR, FPR, TNR, FNR\n",
    "TPR = TP / (TP + FN)\n",
    "FPR = FP / (FP + TN)\n",
    "TNR = TN / (TN + FP)\n",
    "FNR = FN / (FN + TP)\n",
    "\n",
    "# Print TPR, FPR, TNR, FNR\n",
    "print('True Positive Rate (TPR) =', TPR)\n",
    "print('False Positive Rate (FPR) =', FPR)\n",
    "print('True Negative Rate (TNR) =', TNR)\n",
    "print('False Negative Rate (FNR) =', FNR)\n",
    "print('Classification time = ', clf_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:06:09.173906Z",
     "start_time": "2024-03-05T16:04:29.519911700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now start the adaptation process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "import optuna"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:06:09.193161100Z",
     "start_time": "2024-03-05T16:06:09.172395400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S1 RF + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the space of hyperparameters for Random Forest\n",
    "rf_params = {\n",
    "    'n_estimators': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_split': optuna.distributions.IntDistribution(2, 20),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "## Define the space of hyperparameters for SVM\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    rf_hyperparams = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 10, 70),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Initialize and train Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(**rf_hyperparams)\n",
    "    rf_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    rf_predicted = rf_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    rf_precision = precision_score(y_validate_l1, rf_predicted)\n",
    "    svm_precision = precision_score(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    rf_recall = recall_score(y_validate_l1, rf_predicted)\n",
    "    svm_recall = recall_score(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate weighted average of TPR and Precision (you can adjust weights based on preference)\n",
    "    weighted_score = (0.5 * (rf_recall + svm_recall)) + (0.5 * (rf_precision + svm_precision))\n",
    "    \n",
    "    return weighted_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S1 (RF+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "rf_params = {\n",
    "    'n_estimators': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_split': optuna.distributions.IntDistribution(2, 20),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "nbc_params = {\n",
    "    'var_smoothing': optuna.distributions.FloatDistribution(1e-10, 1e-1)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    rf_hyperparams = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 10, 70),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Initialize the rf\n",
    "    rf_classifier = RandomForestClassifier(**rf_hyperparams)\n",
    "    \n",
    "    # Initialize the nbc\n",
    "    nbc_classifier = GaussianNB(**gnb_hyperparams)\n",
    "    \n",
    "    # Initialize and train voting classifier\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[(\"rf\", rf_classifier), (\"nbc\", nbc_classifier)] ,\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    voting_predicted = voting_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    voting_precision = precision_score(y_validate_l1, voting_predicted)\n",
    "    svm_precision = precision_score(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    voting_recall = recall_score(y_validate_l1, voting_predicted)\n",
    "    svm_recall = recall_score(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate weighted average of TPR and Precision (you can adjust weights based on preference)\n",
    "    weighted_score = (0.5 * (voting_recall + svm_recall)) + (0.5 * (voting_precision + svm_precision))\n",
    "    \n",
    "    return weighted_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S1 (HGBC+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "nbc_params = {\n",
    "    'var_smoothing': optuna.distributions.FloatDistribution(1e-10, 1e-1)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for HistGradientBoostingClassifier\n",
    "    hgb_hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('hgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_iter': trial.suggest_int('hgb_max_iter', 50, 500),\n",
    "        'max_depth': trial.suggest_int('hgb_max_depth', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Initialize the hgbc\n",
    "    hgbc_classifier = HistGradientBoostingClassifier(**hgb_hyperparams)\n",
    "    \n",
    "    # Initialize the nbc\n",
    "    nbc_classifier = GaussianNB(**gnb_hyperparams)\n",
    "    \n",
    "    # Initialize and train voting classifier\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[(\"hgbc\", hgbc_classifier), (\"nbc\", nbc_classifier)] ,\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    voting_predicted = voting_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    voting_precision = precision_score(y_validate_l1, voting_predicted)\n",
    "    svm_precision = precision_score(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    voting_recall = recall_score(y_validate_l1, voting_predicted)\n",
    "    svm_recall = recall_score(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate weighted average of TPR and Precision (you can adjust weights based on preference)\n",
    "    weighted_score = (0.5 * (voting_recall + svm_recall)) + (0.5 * (voting_precision + svm_precision))\n",
    "    \n",
    "    return weighted_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S1 HGBC + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for HistGradientBoostingClassifier\n",
    "    hgb_hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('hgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_iter': trial.suggest_int('hgb_max_iter', 50, 500),\n",
    "        'max_depth': trial.suggest_int('hgb_max_depth', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Initialize and train HistGradientBoostingClassifier\n",
    "    hgb_classifier = HistGradientBoostingClassifier(**hgb_hyperparams)\n",
    "    hgb_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    hgb_predicted = hgb_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    hgb_precision = precision_score(y_validate_l1, hgb_predicted)\n",
    "    svm_precision = precision_score(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    hgb_recall = recall_score(y_validate_l1, hgb_predicted)\n",
    "    svm_recall = recall_score(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate weighted average of TPR and Precision (you can adjust weights based on preference)\n",
    "    weighted_score = (0.5 * (hgb_recall + svm_recall)) + (0.5 * (hgb_precision + svm_precision))\n",
    "    \n",
    "    return weighted_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S2 (RF+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "nbc_params = {\n",
    "    'var_smoothing': optuna.distributions.FloatDistribution(1e-10, 1e-1)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    rf_hyperparams = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 10, 70),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Initialize the rf\n",
    "    rf_classifier = RandomForestClassifier(**rf_hyperparams)\n",
    "    \n",
    "    # Initialize the nbc\n",
    "    nbc_classifier = GaussianNB(**gnb_hyperparams)\n",
    "    \n",
    "    # Initialize and train voting classifier\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[(\"rf\", rf_classifier), (\"nbc\", nbc_classifier)] ,\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    voting_predicted = voting_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    voting_cm = confusion_matrix(y_validate_l1, voting_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate False Positive Rates for both classifiers\n",
    "    voting_fpr = voting_cm[0, 1] / np.sum(voting_cm[0, :])\n",
    "    svm_fpr = svm_cm[0, 1] / np.sum(svm_cm[0, :])\n",
    "    \n",
    "    # Calculate the average FPR\n",
    "    avg_fpr = (voting_fpr + svm_fpr) / 2\n",
    "    \n",
    "    return avg_fpr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S2 (HGBC+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for HistGradientBoostingClassifier\n",
    "    hgb_hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('hgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_iter': trial.suggest_int('hgb_max_iter', 50, 500),\n",
    "        'max_depth': trial.suggest_int('hgb_max_depth', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Initialize the rf\n",
    "    hgb_classifier = HistGradientBoostingClassifier(**hgb_hyperparams)\n",
    "    \n",
    "    # Initialize the nbc\n",
    "    nbc_classifier = GaussianNB(**gnb_hyperparams)\n",
    "    \n",
    "    # Initialize and train voting classifier\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[(\"hgbc\", hgb_classifier), (\"nbc\", nbc_classifier)] ,\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    voting_predicted = voting_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    voting_cm = confusion_matrix(y_validate_l1, voting_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate False Positive Rates for both classifiers\n",
    "    voting_fpr = voting_cm[0, 1] / np.sum(voting_cm[0, :])\n",
    "    svm_fpr = svm_cm[0, 1] / np.sum(svm_cm[0, :])\n",
    "    \n",
    "    # Calculate the average FPR\n",
    "    avg_fpr = (voting_fpr + svm_fpr) / 2\n",
    "    \n",
    "    return avg_fpr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S2 HGBC + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for HistGradientBoostingClassifier\n",
    "    hgb_hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('hgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_iter': trial.suggest_int('hgb_max_iter', 50, 500),\n",
    "        'max_depth': trial.suggest_int('hgb_max_depth', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Initialize the rf\n",
    "    hgb_classifier = HistGradientBoostingClassifier(**hgb_hyperparams)\n",
    "    hgb_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    hgb_predicted = hgb_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    hgb_cm = confusion_matrix(y_validate_l1, hgb_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate False Positive Rates for both classifiers\n",
    "    voting_fpr = hgb_cm[0, 1] / np.sum(hgb_cm[0, :])\n",
    "    svm_fpr = svm_cm[0, 1] / np.sum(svm_cm[0, :])\n",
    "    \n",
    "    # Calculate the average FPR\n",
    "    avg_fpr = (voting_fpr + svm_fpr) / 2\n",
    "    \n",
    "    return avg_fpr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S2 RF + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "rf_params = {\n",
    "    'n_estimators': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_split': optuna.distributions.IntDistribution(2, 20),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "nbc_params = {\n",
    "    'var_smoothing': optuna.distributions.FloatDistribution(1e-10, 1e-1)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    rf_hyperparams = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 10, 70),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Initialize and train HistGradientBoostingClassifier\n",
    "    rf_classifier = RandomForestClassifier(**rf_hyperparams)\n",
    "    rf_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    hgb_predicted = rf_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    hgb_cm = confusion_matrix(y_validate_l1, hgb_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate False Positive Rates for both classifiers\n",
    "    rf_fpr = hgb_cm[0, 1] / np.sum(hgb_cm[0, :])\n",
    "    svm_fpr = svm_cm[0, 1] / np.sum(svm_cm[0, :])\n",
    "    \n",
    "    # Calculate the average FPR\n",
    "    avg_fpr = (rf_fpr + svm_fpr) / 2\n",
    "    \n",
    "    return avg_fpr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S3 RF + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import optuna\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_split': optuna.distributions.IntDistribution(2, 20),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    rf_hyperparams = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 10, 70),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Initialize and train Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(**rf_hyperparams)\n",
    "    \n",
    "    # Initialize the NBC\n",
    "    nbc_classifier = GaussianNB(**nbc_params)\n",
    "    \n",
    "    # Initialize and train voting classifier\n",
    "    voting_classifier = VotingClassifier(\n",
    "        estimators=[(\"rf\", rf_classifier), (\"nbc\", nbc_classifier)] ,\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    voting_predicted = voting_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    voting_cm = confusion_matrix(y_validate_l1, voting_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate False Positive Rates (FPR) and True Negative Rates (TNR) for both classifiers\n",
    "    voting_fpr = voting_cm[0, 1] / np.sum(voting_cm[0, :])\n",
    "    voting_tnr = voting_cm[0, 0] / np.sum(voting_cm[0, :])\n",
    "    \n",
    "    svm_fpr = svm_cm[0, 1] / np.sum(svm_cm[0, :])\n",
    "    svm_tnr = svm_cm[0, 0] / np.sum(svm_cm[0, :])\n",
    "    \n",
    "    # Calculate the average FPR and TNR\n",
    "    avg_fpr = (voting_fpr + svm_fpr) / 2\n",
    "    avg_tnr = (voting_tnr + svm_tnr) / 2\n",
    "    \n",
    "    # Aim to optimize both FPR and TNR\n",
    "    return avg_fpr + (1 - avg_tnr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S3 (RF+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import optuna\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_split': optuna.distributions.IntDistribution(2, 20),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "nbc_params = {\n",
    "    'var_smoothing': optuna.distributions.FloatDistribution(1e-10, 1e-1)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    rf_hyperparams = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 10, 70),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    voting_classifier = VotingClassifier(\n",
    "        [(\"rf\", RandomForestClassifier(**rf_hyperparams)), (\"nbc\", GaussianNB(**gnb_hyperparams))],\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    voting_predicted = voting_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    voting_cm = confusion_matrix(y_validate_l1, voting_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate False Positive Rates (FPR) and True Negative Rates (TNR) for both classifiers\n",
    "    rf_fpr = voting_cm[0, 1] / np.sum(voting_cm[0, :])\n",
    "    rf_tnr = voting_cm[0, 0] / np.sum(voting_cm[0, :])\n",
    "    \n",
    "    svm_fpr = svm_cm[0, 1] / np.sum(svm_cm[0, :])\n",
    "    svm_tnr = svm_cm[0, 0] / np.sum(svm_cm[0, :])\n",
    "    \n",
    "    # Calculate the average FPR and TNR\n",
    "    avg_fpr = (rf_fpr + svm_fpr) / 2\n",
    "    avg_tnr = (rf_tnr + svm_tnr) / 2\n",
    "    \n",
    "    # Aim to optimize both FPR and TNR\n",
    "    return avg_fpr + (1 - avg_tnr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S3 (HGBC+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "nbc_params = {\n",
    "    'var_smoothing': optuna.distributions.FloatDistribution(1e-10, 1e-1)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    hgb_hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('hgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_iter': trial.suggest_int('hgb_max_iter', 50, 500),\n",
    "        'max_depth': trial.suggest_int('hgb_max_depth', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    voting_classifier = VotingClassifier(\n",
    "        [(\"hgbc\", HistGradientBoostingClassifier(**hgb_hyperparams)), (\"nbc\", GaussianNB(**gnb_hyperparams))],\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    voting_predicted = voting_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    voting_cm = confusion_matrix(y_validate_l1, voting_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate False Positive Rates (FPR) and True Negative Rates (TNR) for both classifiers\n",
    "    rf_fpr = voting_cm[0, 1] / np.sum(voting_cm[0, :])\n",
    "    rf_tnr = voting_cm[0, 0] / np.sum(voting_cm[0, :])\n",
    "    \n",
    "    svm_fpr = svm_cm[0, 1] / np.sum(svm_cm[0, :])\n",
    "    svm_tnr = svm_cm[0, 0] / np.sum(svm_cm[0, :])\n",
    "    \n",
    "    # Calculate the average FPR and TNR\n",
    "    avg_fpr = (rf_fpr + svm_fpr) / 2\n",
    "    avg_tnr = (rf_tnr + svm_tnr) / 2\n",
    "    \n",
    "    # Aim to optimize both FPR and TNR\n",
    "    return avg_fpr + (1 - avg_tnr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S3 HGBC + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    hgb_hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('hgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_iter': trial.suggest_int('hgb_max_iter', 50, 500),\n",
    "        'max_depth': trial.suggest_int('hgb_max_depth', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    hgb_classifier = HistGradientBoostingClassifier(**hgb_hyperparams)\n",
    "    hgb_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    hgb_predicted = hgb_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    hgb_cm = confusion_matrix(y_validate_l1, hgb_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate False Positive Rates (FPR) and True Negative Rates (TNR) for both classifiers\n",
    "    rf_fpr = hgb_cm[0, 1] / np.sum(hgb_cm[0, :])\n",
    "    rf_tnr = hgb_cm[0, 0] / np.sum(hgb_cm[0, :])\n",
    "    \n",
    "    svm_fpr = svm_cm[0, 1] / np.sum(svm_cm[0, :])\n",
    "    svm_tnr = svm_cm[0, 0] / np.sum(svm_cm[0, :])\n",
    "    \n",
    "    # Calculate the average FPR and TNR\n",
    "    avg_fpr = (rf_fpr + svm_fpr) / 2\n",
    "    avg_tnr = (rf_tnr + svm_tnr) / 2\n",
    "    \n",
    "    # Aim to optimize both FPR and TNR\n",
    "    return avg_fpr + (1 - avg_tnr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S4 HGBC + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    hgb_hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('hgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_iter': trial.suggest_int('hgb_max_iter', 50, 500),\n",
    "        'max_depth': trial.suggest_int('hgb_max_depth', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    hgb_classifier = HistGradientBoostingClassifier(**hgb_hyperparams)\n",
    "    hgb_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    hgb_predicted = hgb_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    hgb_cm = confusion_matrix(y_validate_l1, hgb_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate Precision and False Negative Rate (FNR) for both classifiers\n",
    "    hgb_precision = hgb_cm[1, 1] / np.sum(hgb_cm[:, 1])\n",
    "    hgb_fnr = hgb_cm[1, 0] / np.sum(hgb_cm[1, :])\n",
    "    \n",
    "    svm_precision = svm_cm[1, 1] / np.sum(svm_cm[:, 1])\n",
    "    svm_fnr = svm_cm[1, 0] / np.sum(svm_cm[1, :])\n",
    "    \n",
    "    # Calculate the average Precision and FNR\n",
    "    avg_precision = (hgb_precision + svm_precision) / 2\n",
    "    avg_fnr = (hgb_fnr + svm_fnr) / 2\n",
    "    \n",
    "    # Aim to optimize both Precision and FNR\n",
    "    return 1 - (avg_precision + avg_fnr)  # Minimize the sum of Precision and FNR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S4 (HGBC+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb_params = {\n",
    "    'learning_rate': optuna.distributions.FloatDistribution(0.001, 0.1),\n",
    "    'max_iter': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "nbc_params = {\n",
    "    'var_smoothing': optuna.distributions.FloatDistribution(1e-10, 1e-1)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    hgb_hyperparams = {\n",
    "        'learning_rate': trial.suggest_float('hgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'max_iter': trial.suggest_int('hgb_max_iter', 50, 500),\n",
    "        'max_depth': trial.suggest_int('hgb_max_depth', 2, 32),\n",
    "        'min_samples_leaf': trial.suggest_int('hgb_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    voting_classifier = VotingClassifier(\n",
    "        [(\"hgbc\", HistGradientBoostingClassifier(**hgb_hyperparams)), (\"nbc\", GaussianNB(**gnb_hyperparams))],\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    hgb_classifier = HistGradientBoostingClassifier(**hgb_hyperparams)\n",
    "    hgb_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    hgb_predicted = hgb_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    hgb_cm = confusion_matrix(y_validate_l1, hgb_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate Precision and False Negative Rate (FNR) for both classifiers\n",
    "    hgb_precision = hgb_cm[1, 1] / np.sum(hgb_cm[:, 1])\n",
    "    hgb_fnr = hgb_cm[1, 0] / np.sum(hgb_cm[1, :])\n",
    "    \n",
    "    svm_precision = svm_cm[1, 1] / np.sum(svm_cm[:, 1])\n",
    "    svm_fnr = svm_cm[1, 0] / np.sum(svm_cm[1, :])\n",
    "    \n",
    "    # Calculate the average Precision and FNR\n",
    "    avg_precision = (hgb_precision + svm_precision) / 2\n",
    "    avg_fnr = (hgb_fnr + svm_fnr) / 2\n",
    "    \n",
    "    # Aim to optimize both Precision and FNR\n",
    "    return 1 - (avg_precision + avg_fnr)  # Minimize the sum of Precision and FNR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S4 RF + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import optuna\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_split': optuna.distributions.IntDistribution(2, 20),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    rf_hyperparams = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 10, 70),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    # Initialize and train Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(**rf_hyperparams)\n",
    "    rf_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    rf_predicted = rf_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    rf_cm = confusion_matrix(y_validate_l1, rf_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate Precision and False Negative Rate (FNR) for both classifiers\n",
    "    rf_precision = rf_cm[1, 1] / np.sum(rf_cm[:, 1])\n",
    "    rf_fnr = rf_cm[1, 0] / np.sum(rf_cm[1, :])\n",
    "    \n",
    "    svm_precision = svm_cm[1, 1] / np.sum(svm_cm[:, 1])\n",
    "    svm_fnr = svm_cm[1, 0] / np.sum(svm_cm[1, :])\n",
    "    \n",
    "    # Calculate the average Precision and FNR\n",
    "    avg_precision = (rf_precision + svm_precision) / 2\n",
    "    avg_fnr = (rf_fnr + svm_fnr) / 2\n",
    "    \n",
    "    # Aim to optimize both Precision and FNR\n",
    "    return 1 - (avg_precision + avg_fnr)  # Minimize the sum of Precision and FNR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# S4 (RF+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': optuna.distributions.IntDistribution(50, 500),\n",
    "    'max_depth': optuna.distributions.IntDistribution(2, 32),\n",
    "    'min_samples_split': optuna.distributions.IntDistribution(2, 20),\n",
    "    'min_samples_leaf': optuna.distributions.IntDistribution(1, 10)\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'C': optuna.distributions.FloatDistribution(0.1, 10),\n",
    "    'kernel': optuna.distributions.CategoricalDistribution(['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "    'gamma': optuna.distributions.FloatDistribution(1e-6, 1e-2)\n",
    "}\n",
    "\n",
    "nbc_params = {\n",
    "    'var_smoothing': optuna.distributions.FloatDistribution(1e-10, 1e-1)\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for Random Forest\n",
    "    rf_hyperparams = {\n",
    "        'n_estimators': trial.suggest_int('rf_n_estimators', 10, 70),\n",
    "        'max_depth': trial.suggest_int('rf_max_depth', 2, 32),\n",
    "        'min_samples_split': trial.suggest_int('rf_min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('rf_min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for GaussianNB\n",
    "    gnb_hyperparams = {\n",
    "        'var_smoothing': trial.suggest_float('gnb_var_smoothing', 1e-10, 1e-1, log=True)\n",
    "    }\n",
    "    \n",
    "    # Define hyperparameters for SVM\n",
    "    svm_hyperparams = {\n",
    "        'C': trial.suggest_float('svm_C', 0.1, 10),\n",
    "        'kernel': trial.suggest_categorical('svm_kernel', ['linear', 'poly', 'rbf', 'sigmoid']),\n",
    "        'gamma': trial.suggest_float('svm_gamma', 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    voting_classifier = VotingClassifier(\n",
    "        [(\"rf\", RandomForestClassifier(**rf_hyperparams)), (\"nbc\", GaussianNB(**gnb_hyperparams))],\n",
    "        voting='soft'\n",
    "    )\n",
    "    voting_classifier.fit(X_train_dos_probe, y_train_l1)\n",
    "    \n",
    "    # Initialize and train SVM classifier\n",
    "    svm_classifier = SVC(**svm_hyperparams)\n",
    "    svm_classifier.fit(X_train_r2l_u2r, y_train_l2)\n",
    "    \n",
    "    # Predict on validation set and calculate metrics for both classifiers\n",
    "    voting_predicted = voting_classifier.predict(X_validate_dos_probe)\n",
    "    svm_predicted = svm_classifier.predict(X_validate_r2l_u2r)\n",
    "    \n",
    "    # Calculate confusion matrices for both classifiers\n",
    "    voting_cm = confusion_matrix(y_validate_l1, voting_predicted)\n",
    "    svm_cm = confusion_matrix(y_validate_l2, svm_predicted)\n",
    "    \n",
    "    # Calculate Precision and False Negative Rate (FNR) for both classifiers\n",
    "    voting_precision = voting_cm[1, 1] / np.sum(voting_cm[:, 1])\n",
    "    voting_fnr = voting_cm[1, 0] / np.sum(voting_cm[1, :])\n",
    "    \n",
    "    svm_precision = svm_cm[1, 1] / np.sum(svm_cm[:, 1])\n",
    "    svm_fnr = svm_cm[1, 0] / np.sum(svm_cm[1, :])\n",
    "    \n",
    "    # Calculate the average Precision and FNR\n",
    "    avg_precision = (voting_precision + svm_precision) / 2\n",
    "    avg_fnr = (voting_fnr + svm_fnr) / 2\n",
    "    \n",
    "    # Aim to optimize both Precision and FNR\n",
    "    return 1 - (avg_precision + avg_fnr)  # Minimize the sum of Precision and FNR"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:06:09.215384700Z",
     "start_time": "2024-03-05T16:06:09.182187400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-05 17:06:09,186] A new study created in memory with name: no-name-a248c22a-966d-451f-9338-0984510a6ef6\n",
      "[I 2024-03-05 17:06:21,068] Trial 2 finished with value: 0.43047696577224537 and parameters: {'rf_n_estimators': 19, 'rf_max_depth': 4, 'rf_min_samples_split': 13, 'rf_min_samples_leaf': 1, 'gnb_var_smoothing': 1.235606750844897e-09, 'svm_C': 7.914945389930198, 'svm_kernel': 'sigmoid', 'svm_gamma': 0.006523975880290638}. Best is trial 2 with value: 0.43047696577224537.\n",
      "[I 2024-03-05 17:06:28,719] Trial 3 finished with value: 0.431927596472079 and parameters: {'rf_n_estimators': 10, 'rf_max_depth': 26, 'rf_min_samples_split': 12, 'rf_min_samples_leaf': 1, 'gnb_var_smoothing': 5.241337140110565e-10, 'svm_C': 4.8308487228875014, 'svm_kernel': 'sigmoid', 'svm_gamma': 0.00619964191766539}. Best is trial 2 with value: 0.43047696577224537.\n",
      "[I 2024-03-05 17:06:32,115] Trial 7 finished with value: 0.4293090238638283 and parameters: {'rf_n_estimators': 47, 'rf_max_depth': 3, 'rf_min_samples_split': 11, 'rf_min_samples_leaf': 8, 'gnb_var_smoothing': 1.6372215519197343e-08, 'svm_C': 8.779653814903886, 'svm_kernel': 'rbf', 'svm_gamma': 0.007592736033465652}. Best is trial 7 with value: 0.4293090238638283.\n",
      "[I 2024-03-05 17:06:34,449] Trial 15 finished with value: 0.43042615768323067 and parameters: {'rf_n_estimators': 16, 'rf_max_depth': 32, 'rf_min_samples_split': 7, 'rf_min_samples_leaf': 8, 'gnb_var_smoothing': 0.0033387151846106756, 'svm_C': 5.836035371390859, 'svm_kernel': 'rbf', 'svm_gamma': 0.00920248385578647}. Best is trial 7 with value: 0.4293090238638283.\n",
      "[I 2024-03-05 17:06:42,274] Trial 4 finished with value: 0.4909647294174765 and parameters: {'rf_n_estimators': 63, 'rf_max_depth': 3, 'rf_min_samples_split': 18, 'rf_min_samples_leaf': 5, 'gnb_var_smoothing': 6.734885996068454e-07, 'svm_C': 4.779664872676567, 'svm_kernel': 'poly', 'svm_gamma': 0.001688573195575208}. Best is trial 7 with value: 0.4293090238638283.\n",
      "[I 2024-03-05 17:06:42,354] Trial 0 finished with value: 0.39323550464184076 and parameters: {'rf_n_estimators': 20, 'rf_max_depth': 28, 'rf_min_samples_split': 8, 'rf_min_samples_leaf': 4, 'gnb_var_smoothing': 7.319389313128838e-07, 'svm_C': 7.488191556081674, 'svm_kernel': 'linear', 'svm_gamma': 0.005703074652206596}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:06:42,585] Trial 16 finished with value: 0.4379160308705692 and parameters: {'rf_n_estimators': 12, 'rf_max_depth': 31, 'rf_min_samples_split': 15, 'rf_min_samples_leaf': 1, 'gnb_var_smoothing': 1.9821830732786357e-07, 'svm_C': 9.02423598694654, 'svm_kernel': 'rbf', 'svm_gamma': 0.0010374963647956136}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:06:47,710] Trial 11 finished with value: 0.4510269347529712 and parameters: {'rf_n_estimators': 21, 'rf_max_depth': 27, 'rf_min_samples_split': 3, 'rf_min_samples_leaf': 9, 'gnb_var_smoothing': 1.1236318613683324e-05, 'svm_C': 0.4119848267435282, 'svm_kernel': 'rbf', 'svm_gamma': 0.0004940761723158233}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:06:49,081] Trial 12 finished with value: 0.5015816505231518 and parameters: {'rf_n_estimators': 24, 'rf_max_depth': 22, 'rf_min_samples_split': 13, 'rf_min_samples_leaf': 7, 'gnb_var_smoothing': 4.857323700483582e-10, 'svm_C': 5.516756932722246, 'svm_kernel': 'poly', 'svm_gamma': 0.008009777114215634}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:06:49,597] Trial 18 finished with value: 0.4320800654435265 and parameters: {'rf_n_estimators': 10, 'rf_max_depth': 29, 'rf_min_samples_split': 2, 'rf_min_samples_leaf': 4, 'gnb_var_smoothing': 1.735004687230674e-10, 'svm_C': 7.28318779240317, 'svm_kernel': 'rbf', 'svm_gamma': 0.005583865310901431}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:06:50,178] Trial 17 finished with value: 0.4447983014276141 and parameters: {'rf_n_estimators': 13, 'rf_max_depth': 14, 'rf_min_samples_split': 2, 'rf_min_samples_leaf': 6, 'gnb_var_smoothing': 4.037583325571072e-07, 'svm_C': 4.082892143561654, 'svm_kernel': 'rbf', 'svm_gamma': 0.0037199419029883547}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:06:51,224] Trial 13 finished with value: 0.44246398471527926 and parameters: {'rf_n_estimators': 64, 'rf_max_depth': 4, 'rf_min_samples_split': 7, 'rf_min_samples_leaf': 6, 'gnb_var_smoothing': 2.7217903725010126e-09, 'svm_C': 1.00612521910863, 'svm_kernel': 'rbf', 'svm_gamma': 4.547471182491339e-06}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:06:57,572] Trial 9 finished with value: 0.4276832114789666 and parameters: {'rf_n_estimators': 48, 'rf_max_depth': 8, 'rf_min_samples_split': 9, 'rf_min_samples_leaf': 7, 'gnb_var_smoothing': 7.101466905522657e-06, 'svm_C': 7.971852143716062, 'svm_kernel': 'rbf', 'svm_gamma': 0.009111159050469959}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:07:02,770] Trial 10 finished with value: 0.4526693676453133 and parameters: {'rf_n_estimators': 51, 'rf_max_depth': 9, 'rf_min_samples_split': 15, 'rf_min_samples_leaf': 6, 'gnb_var_smoothing': 3.5607034972913856e-06, 'svm_C': 4.587185548415748, 'svm_kernel': 'rbf', 'svm_gamma': 0.004418456578522179}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:07:07,058] Trial 14 finished with value: 0.40506423152602855 and parameters: {'rf_n_estimators': 48, 'rf_max_depth': 14, 'rf_min_samples_split': 2, 'rf_min_samples_leaf': 7, 'gnb_var_smoothing': 1.9510468118642548e-05, 'svm_C': 9.403154897035122, 'svm_kernel': 'linear', 'svm_gamma': 0.0017825734431676352}. Best is trial 0 with value: 0.39323550464184076.\n",
      "[I 2024-03-05 17:07:08,199] Trial 1 finished with value: 0.38560611955437385 and parameters: {'rf_n_estimators': 44, 'rf_max_depth': 25, 'rf_min_samples_split': 10, 'rf_min_samples_leaf': 3, 'gnb_var_smoothing': 0.013378287127471807, 'svm_C': 4.692742455438723, 'svm_kernel': 'linear', 'svm_gamma': 0.0033254413823773344}. Best is trial 1 with value: 0.38560611955437385.\n",
      "[I 2024-03-05 17:07:11,135] Trial 6 finished with value: 0.5027434904137862 and parameters: {'rf_n_estimators': 49, 'rf_max_depth': 28, 'rf_min_samples_split': 2, 'rf_min_samples_leaf': 8, 'gnb_var_smoothing': 9.996178525895355e-06, 'svm_C': 9.69936006353095, 'svm_kernel': 'poly', 'svm_gamma': 0.005251939387489014}. Best is trial 1 with value: 0.38560611955437385.\n",
      "[I 2024-03-05 17:07:12,799] Trial 5 finished with value: 0.5031875960059983 and parameters: {'rf_n_estimators': 53, 'rf_max_depth': 32, 'rf_min_samples_split': 15, 'rf_min_samples_leaf': 6, 'gnb_var_smoothing': 3.638300299547618e-10, 'svm_C': 6.901092456810841, 'svm_kernel': 'poly', 'svm_gamma': 0.008088109435567753}. Best is trial 1 with value: 0.38560611955437385.\n",
      "[I 2024-03-05 17:07:15,859] Trial 8 finished with value: 0.3881195993027853 and parameters: {'rf_n_estimators': 56, 'rf_max_depth': 27, 'rf_min_samples_split': 10, 'rf_min_samples_leaf': 6, 'gnb_var_smoothing': 0.005381408867258186, 'svm_C': 6.868324248621409, 'svm_kernel': 'linear', 'svm_gamma': 0.002024194772353081}. Best is trial 1 with value: 0.38560611955437385.\n",
      "[I 2024-03-05 17:07:20,093] Trial 19 finished with value: 0.4419359171524119 and parameters: {'rf_n_estimators': 47, 'rf_max_depth': 23, 'rf_min_samples_split': 13, 'rf_min_samples_leaf': 9, 'gnb_var_smoothing': 2.4393769511925106e-05, 'svm_C': 4.935832454825078, 'svm_kernel': 'sigmoid', 'svm_gamma': 0.0058113718031551805}. Best is trial 1 with value: 0.38560611955437385.\n"
     ]
    }
   ],
   "source": [
    "# Create Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "start = datetime.now()\n",
    "study.optimize(objective, n_trials=20, n_jobs=-1)\n",
    "tuning_time = datetime.now() - start"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:07:20.099664400Z",
     "start_time": "2024-03-05T16:06:09.187192800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning_time:  0:01:10.907801\n"
     ]
    }
   ],
   "source": [
    "print('tuning_time: ', tuning_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:07:20.107511Z",
     "start_time": "2024-03-05T16:07:20.099664400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the best hyperparameters for RF + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [],
   "source": [
    "best_params1 = {\n",
    "    'n_estimators': study.best_trial.params['rf_n_estimators'],\n",
    "    'max_depth': study.best_trial.params['rf_max_depth'],\n",
    "    'min_samples_split': study.best_trial.params['rf_min_samples_split'],\n",
    "    'min_samples_leaf': study.best_trial.params['rf_min_samples_leaf']\n",
    "}\n",
    "\n",
    "best_params2 = {\n",
    "    'C': study.best_trial.params['svm_C'],\n",
    "    'kernel': study.best_trial.params['svm_kernel'],\n",
    "    'gamma': study.best_trial.params['svm_gamma']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:07:20.124646100Z",
     "start_time": "2024-03-05T16:07:20.103704Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the best hyperparameters for HGBC + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best_params1 = {\n",
    "    'learning_rate': study.best_trial.params['hgb_learning_rate'],\n",
    "    'max_depth': study.best_trial.params['hgb_max_depth'],\n",
    "    'max_iter': study.best_trial.params['hgb_max_iter'],\n",
    "    'min_samples_leaf': study.best_trial.params['hgb_min_samples_leaf']\n",
    "}\n",
    "\n",
    "best_params2 = {\n",
    "    'C': study.best_trial.params['svm_C'],\n",
    "    'kernel': study.best_trial.params['svm_kernel'],\n",
    "    'gamma': study.best_trial.params['svm_gamma']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the best hyperparameters for (RF+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best_params1 = {\n",
    "    'n_estimators': study.best_trial.params['rf_n_estimators'],\n",
    "    'max_depth': study.best_trial.params['rf_max_depth'],\n",
    "    'min_samples_split': study.best_trial.params['rf_min_samples_split'],\n",
    "    'min_samples_leaf': study.best_trial.params['rf_min_samples_leaf']\n",
    "}\n",
    "\n",
    "best_params_nbc = {\n",
    "    'var_smoothing': study.best_trial.params['gnb_var_smoothing']\n",
    "}\n",
    "\n",
    "best_params2 = {\n",
    "    'C': study.best_trial.params['svm_C'],\n",
    "    'kernel': study.best_trial.params['svm_kernel'],\n",
    "    'gamma': study.best_trial.params['svm_gamma']\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "print(best_params_nbc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the best hyperparameters for (HGBC+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best_params1 = {\n",
    "    'learning_rate': study.best_trial.params['hgb_learning_rate'],\n",
    "    'max_depth': study.best_trial.params['hgb_max_depth'],\n",
    "    'max_iter': study.best_trial.params['hgb_max_iter'],\n",
    "    'min_samples_leaf': study.best_trial.params['hgb_min_samples_leaf']\n",
    "}\n",
    "\n",
    "best_params_nbc = {\n",
    "    'var_smoothing': study.best_trial.params['gnb_var_smoothing']\n",
    "}\n",
    "\n",
    "# Get the best hyperparameters for SVM\n",
    "best_params2 = {\n",
    "    'C': study.best_trial.params['svm_C'],\n",
    "    'kernel': study.best_trial.params['svm_kernel'],\n",
    "    'gamma': study.best_trial.params['svm_gamma']\n",
    "}\n",
    "\n",
    "print(best_params_nbc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 44, 'max_depth': 25, 'min_samples_split': 10, 'min_samples_leaf': 3} {'C': 4.692742455438723, 'kernel': 'linear', 'gamma': 0.0033254413823773344}\n"
     ]
    }
   ],
   "source": [
    "print(best_params1, best_params2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:07:20.138677100Z",
     "start_time": "2024-03-05T16:07:20.109085500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train RF + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [
    {
     "data": {
      "text/plain": "SVC(C=4.692742455438723, gamma=0.0033254413823773344, kernel='linear')",
      "text/html": "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=4.692742455438723, gamma=0.0033254413823773344, kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=4.692742455438723, gamma=0.0033254413823773344, kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier1 = RandomForestClassifier(**best_params1)\n",
    "best_classifier1.fit(X_train_dos_probe, y_train_l1)\n",
    "\n",
    "best_classifier2 = SVC(**best_params2)\n",
    "best_classifier2.fit(X_train_r2l_u2r, y_train_l2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:07:48.886362800Z",
     "start_time": "2024-03-05T16:07:20.112696400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train HGBC + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best_classifier1 = HistGradientBoostingClassifier(**best_params1)\n",
    "best_classifier1.fit(X_train_dos_probe, y_train_l1)\n",
    "\n",
    "best_classifier2 = SVC(**best_params2)\n",
    "best_classifier2.fit(X_train_r2l_u2r, y_train_l2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train (RF+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best_classifier1 = VotingClassifier(\n",
    "    [(\"rf\", RandomForestClassifier(**best_params1)), (\"nbc\", GaussianNB(**best_params_nbc))],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "best_classifier2 = SVC(**best_params2)\n",
    "\n",
    "best_classifier1.fit(X_train_dos_probe, y_train_l1)\n",
    "best_classifier2.fit(X_train_r2l_u2r, y_train_l2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train (HGBC+NBC) + SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best_classifier1 = VotingClassifier(\n",
    "    [(\"hgbc\", HistGradientBoostingClassifier(**best_params1)), (\"nbc\", GaussianNB(**best_params_nbc))],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "best_classifier2 = SVC(**best_params2)\n",
    "\n",
    "best_classifier1.fit(X_train_dos_probe, y_train_l1)\n",
    "best_classifier2.fit(X_train_r2l_u2r, y_train_l2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "classifier1 = best_classifier1\n",
    "classifier2 = best_classifier2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:07:48.893393700Z",
     "start_time": "2024-03-05T16:07:48.885288200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "\n",
    "# Ensemble the classifiers\n",
    "# Suppressing the warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")\n",
    "\n",
    "result = []\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(X_test_layer2.shape[0]):\n",
    "    layer1 = classifier1.predict(X_test_layer1[i].reshape(1, -1))[0]\n",
    "    if layer1 == 1:\n",
    "        result.append(layer1)\n",
    "    else:\n",
    "        layer2 = classifier2.predict(X_test_layer2[i].reshape(1, -1))[0]\n",
    "        if layer2 == 1:\n",
    "            result.append(layer2)\n",
    "        else:\n",
    "            result.append(0)\n",
    "clf_time = datetime.now() - start\n",
    "\n",
    "result_tuned = np.array(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:08:28.872496300Z",
     "start_time": "2024-03-05T16:07:48.889396700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for the full pipeline:\n",
      "[[8997  714]\n",
      " [3025 9808]]\n",
      "Accuracy =  0.8341465578424414\n",
      "F1 Score =  0.8399058017555127\n",
      "Precision =  0.9321421782931002\n",
      "Recall =  0.7642795916777059\n",
      "Matthew corr =  0.685618340680292\n",
      "Classification time =  0:01:10.907801\n"
     ]
    }
   ],
   "source": [
    "# the results may vary\n",
    "# C=0.1, gamma=0.01\n",
    "print('Results for the full pipeline:')\n",
    "print(confusion_matrix(y_test_real, result_tuned))\n",
    "print('Accuracy = ', accuracy_score(y_test_real, result_tuned))\n",
    "print('F1 Score = ', f1_score(y_test_real, result_tuned))\n",
    "print('Precision = ', precision_score(y_test_real, result_tuned))\n",
    "print('Recall = ', recall_score(y_test_real, result))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_test_real, result_tuned))\n",
    "print('Classification time = ', tuning_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:08:28.920876100Z",
     "start_time": "2024-03-05T16:08:28.874498800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate (TPR) = 0.7642795916777059\n",
      "False Positive Rate (FPR) = 0.07352486870559159\n",
      "True Negative Rate (TNR) = 0.9264751312944084\n",
      "False Negative Rate (FNR) = 0.23572040832229407\n",
      "Classification time =  0:00:39.900059\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "TP = FP = TN = FN = 0\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(X_test_layer2.shape[0]):\n",
    "    layer1 = classifier1.predict(X_test_layer1[i].reshape(1, -1))[0]\n",
    "    if layer1 == 1:\n",
    "        result.append(layer1)\n",
    "        if y_test_real[i] == 1:\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "    else:\n",
    "        layer2 = classifier2.predict(X_test_layer2[i].reshape(1, -1))[0]\n",
    "        if layer2 == 1:\n",
    "            result.append(layer2)\n",
    "            if y_test_real[i] == 1:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else:\n",
    "            result.append(0)\n",
    "            if y_test_real[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "clf_time = datetime.now() - start\n",
    "\n",
    "result = np.array(result)\n",
    "\n",
    "# Compute TPR, FPR, TNR, FNR\n",
    "TPR = TP / (TP + FN)\n",
    "FPR = FP / (FP + TN)\n",
    "TNR = TN / (TN + FP)\n",
    "FNR = FN / (FN + TP)\n",
    "\n",
    "# Print TPR, FPR, TNR, FNR\n",
    "print('True Positive Rate (TPR) =', TPR)\n",
    "print('False Positive Rate (FPR) =', FPR)\n",
    "print('True Negative Rate (TNR) =', TNR)\n",
    "print('False Negative Rate (FNR) =', FNR)\n",
    "print('Classification time = ', clf_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:08.821568Z",
     "start_time": "2024-03-05T16:08:28.909750400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ROC graph\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_real, result)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.savefig('static_roc_curve-hgbc.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "if True:\n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_layer1.shape[1] + 1)]\n",
    "    X1_test = pd.DataFrame(data=X_test_layer1, columns=column_names)\n",
    "    X1_test.to_csv('EvalResources/AdditionalSets/x_test_l1_pca.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_layer2.shape[1] + 1)]\n",
    "    X2_test = pd.DataFrame(data=X_test_layer2, columns=column_names)\n",
    "    X2_test.to_csv('EvalResources/AdditionalSets/x_test_l2_pca.txt', index=False)\n",
    "    \n",
    "    np.save('EvalResources/AdditionalSets/y_test', y_test_real)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate seen and unseen attack categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:08.888191400Z",
     "start_time": "2024-03-05T16:09:08.818738900Z"
    }
   },
   "outputs": [],
   "source": [
    "# load testset\n",
    "df_test = pd.read_csv('EvalResources/KDDTest+.txt', sep=\",\", header=None, skipinitialspace = True)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "y_test = df_test['label']\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "\n",
    "df_test_original = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "if EXPORT_DATASETS:\n",
    "    df_test_original.to_csv('EvalResources/ProcessedDatasets/x_test_full.txt', index=False)\n",
    "    np.save('EvalResources/ProcessedDatasets/y_test_full', y_test)\n",
    "    \n",
    "#df_test_original"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:08.891883800Z",
     "start_time": "2024-03-05T16:09:08.884997500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:09.041574700Z",
     "start_time": "2024-03-05T16:09:08.892388700Z"
    }
   },
   "outputs": [],
   "source": [
    "new_attack = []\n",
    "for i in df_test_original['label'].value_counts().index.tolist()[1:]:\n",
    "    if i not in df_train_original['label'].value_counts().index.tolist()[1:]:\n",
    "        new_attack.append(i)\n",
    "        \n",
    "new_attack.sort()\n",
    "#new_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:09.118109700Z",
     "start_time": "2024-03-05T16:09:09.041574700Z"
    }
   },
   "outputs": [],
   "source": [
    "index_of_new_attacks = []\n",
    "\n",
    "for i in range(len(df_test_original)):\n",
    "    if df_test_original['label'][i] in new_attack:\n",
    "        index_of_new_attacks.append(df_test_original.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "#len(index_of_new_attacks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:09.126693500Z",
     "start_time": "2024-03-05T16:09:09.118109700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:09.129948400Z",
     "start_time": "2024-03-05T16:09:09.121612400Z"
    }
   },
   "outputs": [],
   "source": [
    "new_attack.append('normal')\n",
    "#new_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:09.203314900Z",
     "start_time": "2024-03-05T16:09:09.125174500Z"
    }
   },
   "outputs": [],
   "source": [
    "index_of_old_attacks = []\n",
    "\n",
    "for i in range(len(df_test_original)):\n",
    "    if df_test_original['label'][i] not in new_attack:\n",
    "        index_of_old_attacks.append(df_test_original.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:09.210916400Z",
     "start_time": "2024-03-05T16:09:09.204824500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new attacks in the test set:  3750\n",
      "Number of new attacks detected by the classifiers:  2089\n",
      "Proportion of new attacks detected:  0.5570666666666667\n"
     ]
    }
   ],
   "source": [
    "print('Number of new attacks in the test set: ', result[index_of_new_attacks].shape[0])\n",
    "print('Number of new attacks detected by the classifiers: ', result[index_of_new_attacks].sum())\n",
    "print('Proportion of new attacks detected: ', result[index_of_new_attacks].sum()/result[index_of_new_attacks].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:09.238307800Z",
     "start_time": "2024-03-05T16:09:09.210916400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of old attacks in the test set:  9083\n",
      "Number of old attacks detected by the classifiers:  7719\n",
      "Proportion of old attacks detected:  0.8498293515358362\n"
     ]
    }
   ],
   "source": [
    "print('Number of old attacks in the test set: ', result[index_of_old_attacks].shape[0])\n",
    "print('Number of old attacks detected by the classifiers: ', result[index_of_old_attacks].sum())\n",
    "print('Proportion of old attacks detected: ', result[index_of_old_attacks].sum()/result[index_of_old_attacks].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate single attack types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T16:09:09.333556400Z",
     "start_time": "2024-03-05T16:09:09.220696400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation split into single attack type:\n",
      "Number of dos attacks:  7460\n",
      "Number of detected attacks:  6244\n",
      "Ratio of detection:  0.8369973190348525\n",
      "Number of probe attacks:  2421\n",
      "Number of detected attacks:  2310\n",
      "Ratio of detection:  0.9541511771995044\n",
      "Number of r2l attacks:  2885\n",
      "Number of detected attacks:  1198\n",
      "Ratio of detection:  0.4152512998266898\n",
      "Number of u2r attacks:  67\n",
      "Number of detected attacks:  56\n",
      "Ratio of detection:  0.835820895522388\n"
     ]
    }
   ],
   "source": [
    "# load test set\n",
    "df_test = pd.read_csv('EvalResources/KDDTest+.txt', sep=\",\", header=None, skipinitialspace = True)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "y_test = df_test['label']\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "df_test_original = df_test\n",
    "df = df_test_original\n",
    "\n",
    "dos_index = df.index[(df['label'].isin(dos_attacks))].tolist()\n",
    "probe_index = df.index[(df['label'].isin(probe_attacks))].tolist()\n",
    "r2l_index = df.index[(df['label'].isin(r2l_attacks))].tolist()\n",
    "u2r_index = df.index[(df['label'].isin(u2r_attacks))].tolist()\n",
    "\n",
    "print('Evaluation split into single attack type:')\n",
    "print(\"Number of dos attacks: \", result[dos_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[dos_index].sum())\n",
    "print(\"Ratio of detection: \", result[dos_index].sum()/result[dos_index].shape[0])\n",
    "\n",
    "print(\"Number of probe attacks: \", result[probe_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[probe_index].sum())\n",
    "print(\"Ratio of detection: \", result[probe_index].sum()/result[probe_index].shape[0])\n",
    "\n",
    "print(\"Number of r2l attacks: \", result[r2l_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[r2l_index].sum())\n",
    "print(\"Ratio of detection: \", result[r2l_index].sum()/result[r2l_index].shape[0])\n",
    "\n",
    "print(\"Number of u2r attacks: \", result[u2r_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[u2r_index].sum())\n",
    "print(\"Ratio of detection: \", result[u2r_index].sum()/result[u2r_index].shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
