{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T10:30:23.276820100Z",
     "start_time": "2023-12-11T10:30:20.750269400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.under_sampling import RandomUnderSampler as under_sam\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import matthews_corrcoef, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ICFS function\n",
    "Takes a dataframe as parameter and saves to file all the features necessary to describe DoS+Probe and U2R+R2L"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def pearson_correlated_features(x, y, threshold):\n",
    "    y['target'] = y['target'].astype(int)\n",
    "\n",
    "    for p in x.columns:\n",
    "        x[p] = x[p].astype(float)\n",
    "\n",
    "    # Ensure y is a DataFrame for consistency\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = pd.DataFrame(y, columns=['target'])\n",
    "\n",
    "    # Calculate the Pearson's correlation coefficients between features and the target variable(s)\n",
    "    corr_matrix = x.corrwith(y['target'])\n",
    "\n",
    "    # Select features with correlations above the threshold\n",
    "    selected_features = x.columns[corr_matrix.abs() > threshold].tolist()\n",
    "\n",
    "    return selected_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T10:30:23.295336Z",
     "start_time": "2023-12-11T10:30:23.255020500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def compute_set_difference(df1, df2):\n",
    "    # Create a new DataFrame containing the set difference of the two DataFrames.\n",
    "    df_diff = df1[~df1.index.isin(df2.index)]\n",
    "    # Return the DataFrame.\n",
    "    return df_diff"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T10:30:23.354047500Z",
     "start_time": "2023-12-11T10:30:23.263380800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def perform_icfs(x_train):\n",
    "    # now ICFS only on the numerical features\n",
    "    num_train = copy.deepcopy(x_train)\n",
    "    del num_train['protocol_type']\n",
    "    del num_train['service']\n",
    "    del num_train['flag']\n",
    "\n",
    "    target = pd.DataFrame()\n",
    "    target['target'] = np.array([1 if x != 'normal' else 0 for x in num_train['label']])\n",
    "    num_train = pd.concat([num_train, target], axis=1)\n",
    "\n",
    "    # These are how attacks are categorized in the trainset\n",
    "    dos_list = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop']\n",
    "    probe_list = ['ipsweep', 'portsweep', 'satan', 'nmap']\n",
    "    u2r_list = ['loadmodule', 'perl', 'rootkit', 'buffer_overflow']\n",
    "    r2l_list = ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'phf', 'spy', 'warezclient', 'warezmaster']\n",
    "    normal = ['normal']\n",
    "\n",
    "    # useful sub-sets\n",
    "    x_normal = num_train[num_train['label'].isin(normal)]\n",
    "    x_u2r = num_train[num_train['label'].isin(u2r_list)]\n",
    "    x_r2l = num_train[num_train['label'].isin(r2l_list)]\n",
    "    x_dos = num_train[num_train['label'].isin(dos_list)]\n",
    "    x_probe = num_train[num_train['label'].isin(probe_list)]\n",
    "\n",
    "    # start the ICFS with l1\n",
    "\n",
    "    # features for dos\n",
    "    dos = copy.deepcopy(num_train)\n",
    "    del dos['target']\n",
    "    y = np.array([1 if x in dos_list else 0 for x in dos['label']])\n",
    "    y_dos = pd.DataFrame(y, columns=['target'])\n",
    "    del dos['label']\n",
    "    dos_all = pearson_correlated_features(dos, y_dos, 0.1)\n",
    "    print(dos_all)\n",
    "\n",
    "    # features for probe\n",
    "    probe = copy.deepcopy(num_train)\n",
    "    del probe['target']\n",
    "    y = np.array([1 if x in probe_list else 0 for x in probe['label']])\n",
    "    y_probe = pd.DataFrame(y, columns=['target'])\n",
    "    del probe['label']\n",
    "    probe_all = pearson_correlated_features(probe, y_probe, 0.1)\n",
    "    print(probe_all)\n",
    "\n",
    "    # intersect for the optimal features\n",
    "    set_dos = set(dos_all)\n",
    "    set_probe = set(probe_all)\n",
    "\n",
    "    comm_features_l1 = set_probe & set_dos\n",
    "\n",
    "    print('common features to train l1: ', comm_features_l1)\n",
    "\n",
    "    # now l2 needs the features to describe the difference between rare attacks and normal traffic\n",
    "\n",
    "    # features for u2r\n",
    "    u2r = pd.concat([x_u2r, x_normal], axis=0)\n",
    "    del u2r['target']\n",
    "    y = np.array([1 if x in u2r_list else 0 for x in u2r['label']])\n",
    "    y_u2r = pd.DataFrame(y, columns=['target'])\n",
    "    del u2r['label']\n",
    "    u2r_all = pearson_correlated_features(u2r, y_u2r, 0.01)\n",
    "    print(u2r_all)\n",
    "\n",
    "    # features for r2l\n",
    "    r2l = pd.concat([x_r2l, x_normal], axis=0)\n",
    "    del r2l['target']\n",
    "    y = np.array([1 if x in r2l_list else 0 for x in r2l['label']])\n",
    "    y_r2l = pd.DataFrame(y, columns=['target'])\n",
    "    del r2l['label']\n",
    "    r2l_all = pearson_correlated_features(r2l, y_r2l, 0.01)\n",
    "    print(r2l_all)\n",
    "\n",
    "    # intersect for the optimal features\n",
    "    set_r2l = set(r2l_all)\n",
    "    set_u2r = set(u2r_all)\n",
    "\n",
    "    comm_features_l2 = set_r2l & set_u2r\n",
    "    # print('Common features to train l2: ', len(common_features_l2), common_features_l2)\n",
    "\n",
    "    with open('NSL-KDD Files/NSL_features_l1.txt', 'w') as g:\n",
    "        for a, x in enumerate(comm_features_l1):\n",
    "            if a < len(comm_features_l1) - 1:\n",
    "                g.write(x + ',' + '\\n')\n",
    "            else:\n",
    "                g.write(x)\n",
    "\n",
    "    # read the common features from file\n",
    "    with open('NSL-KDD Files/NSL_features_l2.txt', 'w') as g:\n",
    "        for a, x in enumerate(comm_features_l2):\n",
    "            if a < len(comm_features_l2) - 1:\n",
    "                g.write(x + ',' + '\\n')\n",
    "            else:\n",
    "                g.write(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T10:30:23.402418300Z",
     "start_time": "2023-12-11T10:30:23.284377900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'KB Process/NSL-KDD Original Datasets/KDDTrain+_20Percent.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# loading the train 20% set\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df_train20 \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mKB Process/NSL-KDD Original Datasets/KDDTrain+_20Percent.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m df_train20 \u001B[38;5;241m=\u001B[39m df_train20[df_train20\u001B[38;5;241m.\u001B[39mcolumns[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]]  \u001B[38;5;66;03m# tags column\u001B[39;00m\n\u001B[0;32m      4\u001B[0m titles \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKB Process/NSL-KDD Original Datasets/Field Names.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, header\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    900\u001B[0m     dialect,\n\u001B[0;32m    901\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    909\u001B[0m )\n\u001B[0;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\PycharmProjects\\IDS_Thesis\\venv\\Lib\\site-packages\\pandas\\io\\common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'KB Process/NSL-KDD Original Datasets/KDDTrain+_20Percent.txt'"
     ]
    }
   ],
   "source": [
    "# loading the train 20% set\n",
    "df_train20 = pd.read_csv('KB Process/NSL-KDD Original Datasets/KDDTrain+_20Percent.txt', sep=\",\", header=None)\n",
    "df_train20 = df_train20[df_train20.columns[:-1]]  # tags column\n",
    "titles = pd.read_csv('KB Process/NSL-KDD Original Datasets/Field Names.csv', header=None)\n",
    "label = pd.Series(['label'], index=[41])\n",
    "titles = pd.concat([titles[0], label])\n",
    "df_train20.columns = titles.to_list()\n",
    "df_train20 = df_train20.drop(['num_outbound_cmds'],axis=1)\n",
    "df_train_original = df_train20\n",
    "\n",
    "# df_train_original.to_csv('KB Process/NSL-KDD Original Datasets/KDDTrain+20_percent_with_labels.txt', index=False)\n",
    "\n",
    "df_train_original"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T10:30:24.136275900Z",
     "start_time": "2023-12-11T10:30:23.292071700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.132275300Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading the train set\n",
    "df_train = pd.read_csv('KB Process/NSL-KDD Original Datasets/KDDTrain+.txt', sep=\",\", header=None)\n",
    "df_train = df_train[df_train.columns[:-1]]  # tags column\n",
    "titles = pd.read_csv('KB Process/NSL-KDD Original Datasets/Field Names.csv', header=None)\n",
    "label = pd.Series(['label'], index=[41])\n",
    "titles = pd.concat([titles[0], label])\n",
    "df_train.columns = titles.to_list()\n",
    "df_train = df_train.drop(['num_outbound_cmds'],axis=1)\n",
    "df_train_original = df_train\n",
    "\n",
    "# df_train_original.to_csv('KB Process/NSL-KDD Original Datasets/KDDTrain+_with_labels.txt', index=False)\n",
    "\n",
    "df_train_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# EDA Plots\n",
    "\n",
    "data = copy.deepcopy(df_train_original)\n",
    "\n",
    "# Separate features and labels\n",
    "features = data.drop('label', axis=1)\n",
    "labels = data['label']\n",
    "\n",
    "# Build the major category attacks array\n",
    "attacks = []\n",
    "for item in labels:\n",
    "    if item in [\"back\", \"land\", \"neptune\", \"pod\", \"smurf\", \"teardrop\"]:\n",
    "        attacks.append(\"dos\")\n",
    "    elif item in [\"ipsweep\", \"nmap\", \"portsweep\", \"satan\"]:\n",
    "        attacks.append(\"probe\")\n",
    "    elif item in [\"ftp_write\", \"guess_passwd\", \"imap\", \"multihop\", \"phf\", \"spy\", \"warezmaster\", \"warezclient\"]:\n",
    "        attacks.append(\"r2l\")\n",
    "    elif item in [\"buffer_overflow\", \"loadmodule\", \"perl\", \"rootkit\"]:\n",
    "        attacks.append(\"u2r\")\n",
    "    else: \n",
    "        attacks.append(\"normal\")\n",
    "        \n",
    "# Plot histograms\n",
    "df_hist = copy.deepcopy(features)\n",
    "df_hist['attack_cat'] = attacks\n",
    "df_hist['labels'] = labels\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_hist['attack_cat'], bins=10, color='skyblue', align='mid')\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('')\n",
    "plt.title('')\n",
    "plt.savefig('Attacks histogram')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.134273200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "df_hist = copy.deepcopy(features)\n",
    "df_hist['attack_cat'] = attacks\n",
    "df_hist['labels'] = labels\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df_hist['labels'], bins=60, color='red', align='mid')\n",
    "plt.xlabel('')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('')\n",
    "plt.title('')\n",
    "plt.savefig('Sub-attacks histogram')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.134273200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop the categorical features\n",
    "del features['flag']\n",
    "del features['protocol_type']\n",
    "del features['service']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "pca_df['label'] = labels\n",
    "pca_df['attacks'] = attacks\n",
    "\n",
    "# Plot the 2D PCA visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = {'normal': 'blue', 'dos': 'orange', 'probe': 'green', 'u2r': 'red', 'r2l': 'purple'}\n",
    "\n",
    "for attack_class, color in colors.items():\n",
    "    subset = pca_df[pca_df['attacks'] == attack_class]\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=attack_class, color=color, alpha=0.7)\n",
    "\n",
    "plt.title('2D PCA Visualization of NSL-KDD Attack Classes + Normal')\n",
    "plt.xlabel('Principal Component 1 (PC1)')\n",
    "plt.ylabel('Principal Component 2 (PC2)')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('PCA Visualization')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.134273200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Same plot without normal instances\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = {'dos': 'orange', 'probe': 'green', 'u2r': 'red', 'r2l': 'purple'}\n",
    "\n",
    "for attack_class, color in colors.items():\n",
    "    subset = pca_df[pca_df['attacks'] == attack_class]\n",
    "    plt.scatter(subset['PC1'], subset['PC2'], label=attack_class, color=color, alpha=0.7)\n",
    "\n",
    "plt.title('2D PCA Visualization of NSL-KDD Attack Classes')\n",
    "plt.xlabel('Principal Component 1 (PC1)')\n",
    "plt.ylabel('Principal Component 2 (PC2)')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('PCA Visualization no normal')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.135275100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.135275100Z"
    }
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "df_test = pd.read_csv('KB Process/NSL-KDD Original Datasets/KDDTest+.txt', sep=\",\", header=None)\n",
    "df_test_ = df_test.sort_index(axis=1)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "df_test_original = df_test\n",
    "\n",
    "# df_test_original.to_csv('KB Process/NSL-KDD Original Datasets/KDDTest+.txt', index=False)\n",
    "\n",
    "df_test_original"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Execution Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EXPORT_MODELS = 0\n",
    "EXPORT_DATASETS = 0\n",
    "EXPORT_PCA = 0\n",
    "EXPORT_ENCODERS = 0\n",
    "\n",
    "pd.options.display.max_columns = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.135275100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform ICFS if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# It is possible to compute the ICFS again\n",
    "\n",
    "# perform_icfs(df_train_original)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T10:30:24.189574600Z",
     "start_time": "2023-12-11T10:30:24.138273400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoS + Probe classifier (NBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# list of single attacks \n",
    "dos_attacks = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop', 'worm', 'apache2', 'mailbomb', 'processtable', 'udpstorm']\n",
    "probe_attacks = ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']\n",
    "r2l_attacks = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster',\n",
    "                'snmpguess', 'spy', 'warezclient', 'httptunnel', 'named', 'sendmail', 'snmpgetattack', 'xlock', 'xsnoop']\n",
    "u2r_attacks = ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm'] \n",
    "\n",
    "# list of attack classes split according to detection layer\n",
    "dos_probe_list = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop', 'ipsweep', 'nmap', 'portsweep', 'satan']\n",
    "dos_probe_test = ['apache2', 'mailbomb', 'processtable', 'udpstorm', 'mscan', 'saint']\n",
    "u2r_r2l_list = ['guess_passwd', 'ftp_write', 'imap', 'phf', 'multihop', 'warezmaster',\n",
    "                'snmpguess', 'spy', 'warezclient', 'buffer_overflow', 'loadmodule', 'rootkit', 'perl']\n",
    "u2r_r2l_test = ['httptunnel', 'named', 'sendmail', 'snmpgetattack', 'xlock', 'xsnoop', 'ps', 'xterm', 'sqlattack']\n",
    "normal_list = ['normal']\n",
    "categorical_features = ['protocol_type', 'service', 'flag']\n",
    "\n",
    "# load the features obtained with ICFS for both layer 1 and layer 2\n",
    "with open('KB Process/Required Files/NSL_features_l1.txt', 'r') as f:\n",
    "    common_features_l1 = f.read().split(',')\n",
    "\n",
    "with open('KB Process/Required Files/NSL_features_l2.txt', 'r') as f:\n",
    "    common_features_l2 = f.read().split(',')\n",
    "    \n",
    "df_train_and_validate = copy.deepcopy(df_train_original)\n",
    "df_test = copy.deepcopy(df_test_original)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.138273400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.139277300Z"
    }
   },
   "outputs": [],
   "source": [
    "# split in test and validation set for BOTH layers\n",
    "df_train_original, df_val_original = train_test_split(df_train_and_validate, test_size=0.2, random_state=42)\n",
    "\n",
    "# dataframes specifically for layer 1\n",
    "df_train = copy.deepcopy(df_train_original)\n",
    "df_val = copy.deepcopy(df_val_original)\n",
    "\n",
    "# set the target variables accordingly\n",
    "y_train = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_train['label']])\n",
    "y_val = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_val['label']])\n",
    "\n",
    "# this dataframe contains the whole train set \n",
    "df_train = df_train.drop(['label'],axis=1)\n",
    "df_train = df_train.reset_index().drop(['index'], axis=1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this dataframe contains the whole validation set\n",
    "df_val = df_val.drop(['label'],axis=1)\n",
    "df_val = df_val.reset_index().drop(['index'], axis=1)\n",
    "\n",
    "df_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.139277300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.139277300Z"
    }
   },
   "outputs": [],
   "source": [
    "# now the real processing for layer 1 starts\n",
    "X_train = df_train[common_features_l1]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_validate = df_val[common_features_l1]\n",
    "X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.140275800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2 one-hot encoders, one for the features of layer1 and one for the features of layer2\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe2 = OneHotEncoder(handle_unknown='ignore')\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.140275800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.142273700Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaling the train set for layer1\n",
    "df_minmax = scaler1.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(df_minmax, columns=X_train.columns)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# scaling the validation set for layer1\n",
    "df_minmax_val = scaler1.transform(X_validate)\n",
    "X_validate = pd.DataFrame(df_minmax_val, columns=X_validate.columns)\n",
    "\n",
    "X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.145680500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.149490500Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the train set\n",
    "label_enc = ohe.fit_transform(df_train[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_train = pd.concat([X_train, df_enc], axis=1)\n",
    "\n",
    "df_train.iloc[:,1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the validation set\n",
    "label_enc = ohe.transform(df_val[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_validate = pd.concat([X_validate, df_enc], axis=1)\n",
    "\n",
    "X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.152722900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.155893500Z"
    }
   },
   "outputs": [],
   "source": [
    "# do the same for testset\n",
    "y_test = np.array([1 if x in (dos_attacks+probe_attacks) else 0 for x in df_test['label']])\n",
    "\n",
    "df_test = df_test.drop(['label'],axis=1)\n",
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.158906600Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test[common_features_l1]\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.162917100Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax = scaler1.transform(X_test)\n",
    "X_test = pd.DataFrame(df_minmax, columns=X_test.columns)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.165922600Z"
    }
   },
   "outputs": [],
   "source": [
    "label_enc = ohe.transform(df_test.iloc[:,1:4])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test = pd.concat([X_test, df_enc], axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.169027700Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Shape of the whole train set: ', X_train.shape)\n",
    "print('Shape of its targets: ', y_train.shape)\n",
    "print('Shape of the whole test set: ', X_test.shape)\n",
    "print('Shape of its targets: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export the dataset for training layer 1\n",
    "if EXPORT_DATASETS:\n",
    "    X_train.to_csv('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDTrain+_l1.txt', index=False)\n",
    "    X_validate.to_csv('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDValidate+_l1.txt', index=False)\n",
    "    np.save('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDTrain+_l1_targets', y_train)\n",
    "    np.save('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDValidate+_l1_targets', y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.172248700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Principal Component Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_dos_probe = PCA(n_components=0.95)\n",
    "X_train_dos_probe = pca_dos_probe.fit_transform(X_train)\n",
    "X_test_dos_probe = pca_dos_probe.transform(X_test)\n",
    "X_validate_dos_probe = pca_dos_probe.transform(X_validate)\n",
    "\n",
    "# X_train = X_train.sort_index(axis=1)\n",
    "X_train_dos_probe.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.175631500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if EXPORT_PCA:\n",
    "    # save the pca transformed as well as the transformer\n",
    "    joblib.dump(X_test_dos_probe, 'KB Process/NSL-KDD Encoded Datasets/pca_transformed/KDDTest+_l1_pca.pkl')\n",
    "    joblib.dump(X_train_dos_probe, 'KB Process/NSL-KDD Encoded Datasets/pca_transformed/KDDTrain+_l1_pca.pkl')\n",
    "    joblib.dump(X_validate_dos_probe, 'KB Process/NSL-KDD Encoded Datasets/pca_transformed/KDDValidate+_l1_pca.pkl')\n",
    "    joblib.dump(pca_dos_probe, 'KB Process/NSL-KDD Encoded Datasets/pca_transformed/layer1_pca_transformer.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.178617200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building the classifier for the layer1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Using Random Forest Classifier\n",
    "# dos_probe_classifier = RandomForestClassifier(n_estimators=100, criterion='gini')\n",
    "\n",
    "# Using the Naive Bayes Classifier\n",
    "dos_probe_classifier = GaussianNB()\n",
    "dos_probe_classifier.fit(X_train_dos_probe, y_train)\n",
    "predicted = dos_probe_classifier.predict(X_test_dos_probe)\n",
    "# class_probabilities = dos_probe_classifier.predict_proba(X_test_dos_probe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.181883400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.185203500Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Metrics for layer 1:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Shape of the train set for l1: ', X_train_dos_probe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2L+U2R classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.188574500Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = copy.deepcopy(df_train_original)\n",
    "df_test = copy.deepcopy(df_test_original)\n",
    "df_val = copy.deepcopy(df_val_original)\n",
    "\n",
    "# load targeted attacks (Normal + r2l + u2r)\n",
    "df_train = df_train[df_train['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "df_val = df_val[df_val['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "\n",
    "# set the target variables accordingly\n",
    "y_train = np.array([1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_train['label']])\n",
    "y_val = np.array([1 if x in (u2r_attacks+r2l_attacks) else 0 for x in df_val['label']])\n",
    "\n",
    "df_train = df_train.drop(['label'],axis=1)\n",
    "df_train = df_train.reset_index().drop(['index'], axis=1)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_val = df_val.drop(['label'],axis=1)\n",
    "df_val = df_val.reset_index().drop(['index'], axis=1)\n",
    "df_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-11T10:30:24.717693100Z",
     "start_time": "2023-12-11T10:30:24.191572900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.194572400Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train[common_features_l2]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_validate = df_val[common_features_l2]\n",
    "X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.197574900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.200575400Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax = scaler2.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(df_minmax, columns=X_train.columns)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_minmax = scaler2.transform(X_validate)\n",
    "X_validate = pd.DataFrame(df_minmax, columns=X_validate.columns)\n",
    "X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.202573900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.205576900Z"
    }
   },
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the train set\n",
    "label_enc = ohe2.fit_transform(df_train[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_train = pd.concat([X_train, df_enc], axis=1)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# perform One-hot encoding for the validation set\n",
    "label_enc = ohe2.transform(df_val[categorical_features])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_validate = pd.concat([X_validate, df_enc], axis=1)\n",
    "X_validate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.209576600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.211576400Z"
    }
   },
   "outputs": [],
   "source": [
    "# do the same for test set\n",
    "df_test = df_test[df_test['label'].isin(normal_list+u2r_attacks+r2l_attacks)]\n",
    "\n",
    "y_test = np.array([0 if x=='normal' else 1 for x in df_test['label']])\n",
    "df_test = df_test.drop(['label'],axis=1)\n",
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.214576Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test[common_features_l2] \n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.216575800Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax = scaler2.transform(X_test)\n",
    "X_test = pd.DataFrame(df_minmax, columns=X_test.columns)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.219576Z"
    }
   },
   "outputs": [],
   "source": [
    "label_enc = ohe2.transform(df_test.iloc[:,1:4])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test = pd.concat([X_test, df_enc], axis=1)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.222575200Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Shape of the train set: ', X_train.shape)\n",
    "print('Shape of its target: ', y_train.shape)\n",
    "print('Shape of the test set: ', X_test.shape)\n",
    "print('Shape of its target: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Under sampling the train set for l2\n",
    "sm = under_sam(sampling_strategy=1)\n",
    "X_train, y_train = sm.fit_resample(X_train,y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.225576100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export the datasets\n",
    "Train set has been scaled, one hot encoded, undersampled\n",
    "Test set has been scaled and one hot encoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export the dataset for training layer 2\n",
    "if EXPORT_DATASETS:\n",
    "    X_train.to_csv('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDTrain+_l2.txt', index=False)\n",
    "    np.save('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDTrain+_l2_targets', y_train)\n",
    "    X_validate.to_csv('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDValidate+_l2.txt', index=False)\n",
    "    np.save('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDValidate+_l2_targets', y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.227576600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Principal Component Analysis\n",
    "pca_r2l_u2r = PCA(n_components=0.95)\n",
    "X_train_r2l_u2r = pca_r2l_u2r.fit_transform(X_train)\n",
    "X_test_r2l_u2r = pca_r2l_u2r.transform(X_test)\n",
    "X_validate_r2l_u2r = pca_r2l_u2r.transform(X_validate)\n",
    "\n",
    "# Try also Decision Trees\n",
    "# r2l_u2r_classifier = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Support Vector Machine for layer l2\n",
    "r2l_u2r_classifier = SVC(C=0.1, gamma=0.01, kernel='rbf', probability=True)\n",
    "r2l_u2r_classifier.fit(X_train_r2l_u2r, y_train)\n",
    "predicted = r2l_u2r_classifier.predict(X_test_r2l_u2r)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.230576100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if EXPORT_PCA:\n",
    "    # save the pca transformed as well as the transformer\n",
    "    joblib.dump(X_test_r2l_u2r, 'KB Process/NSL-KDD Encoded Datasets/pca_transformed/KDDTest+_l2_pca.pkl')\n",
    "    joblib.dump(X_train_r2l_u2r, 'KB Process/NSL-KDD Encoded Datasets/pca_transformed/KDDTrain+_l2_pca.pkl')\n",
    "    joblib.dump(X_train_r2l_u2r, 'KB Process/NSL-KDD Encoded Datasets/pca_transformed/KDDValidate+_l2_pca.pkl')\n",
    "    joblib.dump(pca_r2l_u2r, 'KB Process/NSL-KDD Encoded Datasets/pca_transformed/layer2_pca_transformer.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.233576100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Metrics for layer 2:')\n",
    "print('Confusion matrix: [TP FN / FP TN]\\n', confusion_matrix(y_test,predicted))\n",
    "print('Accuracy = ', accuracy_score(y_test,predicted))\n",
    "print('F1 Score = ', f1_score(y_test,predicted))\n",
    "print('Precision = ', precision_score(y_test,predicted))\n",
    "print('Recall = ', recall_score(y_test,predicted))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_test,predicted))\n",
    "print('Shape of the training set: ', X_train_r2l_u2r.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.236573400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the classifiers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if EXPORT_MODELS:\n",
    "    with open('KB Process/Models/From notebook/NSL_l1_classifier.pkl', \"wb\") as f:\n",
    "        pickle.dump(dos_probe_classifier, f)\n",
    "    with open('KB Process/Models/From notebook/NSL_l2_classifier.pkl', \"wb\") as f:\n",
    "        pickle.dump(r2l_u2r_classifier, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.239068600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.241179500Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test1 = copy.deepcopy(df_test_original)\n",
    "df_test2 = copy.deepcopy(df_test_original)\n",
    "y_test_real = np.array([0 if x=='normal' else 1 for x in df_test1['label']])\n",
    "df_test_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test = df_test1[common_features_l1]\n",
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.242321900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.244655600Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax = scaler1.transform(X_test)\n",
    "X_test = pd.DataFrame(df_minmax, columns=X_test.columns)\n",
    "label_enc = ohe.transform(df_test1.iloc[:,1:4])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test = pd.concat([X_test, df_enc], axis=1)\n",
    "\n",
    "X_test_layer1 = pca_dos_probe.transform(X_test)\n",
    "print('Test set shape for layer 1: ', X_test_layer1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test = df_test2[common_features_l2] \n",
    "X_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.246673700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.247673900Z"
    }
   },
   "outputs": [],
   "source": [
    "df_minmax = scaler2.transform(X_test)\n",
    "X_test = pd.DataFrame(df_minmax, columns=X_test.columns)\n",
    "label_enc = ohe2.transform(df_test2.iloc[:,1:4])\n",
    "label_enc.toarray()\n",
    "new_labels = ohe2.get_feature_names_out(categorical_features)\n",
    "df_enc = pd.DataFrame(data=label_enc.toarray(), columns=new_labels)\n",
    "X_test = pd.concat([X_test, df_enc], axis=1)\n",
    "\n",
    "X_test_layer2 = pca_r2l_u2r.transform(X_test)\n",
    "print('Test set shape for layer 2: ', X_test_layer2.shape)\n",
    "print('Type of X_test_layer1: ', type(X_test_layer1))\n",
    "print('Type of X_test_layer1: ', type(X_test_layer2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.249674300Z"
    }
   },
   "outputs": [],
   "source": [
    "# same classifiers obtained above\n",
    "classifier1 = dos_probe_classifier\n",
    "classifier2 = r2l_u2r_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.251670600Z"
    }
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(X_test_layer2.shape[0]):\n",
    "    layer1 = classifier1.predict(X_test_layer1[i].reshape(1, -1))[0]\n",
    "    if layer1 == 1:\n",
    "        result.append(layer1)\n",
    "    else:\n",
    "        layer2 = classifier2.predict(X_test_layer2[i].reshape(1, -1))[0]\n",
    "        if layer2 == 1:\n",
    "            result.append(layer2)\n",
    "        else:\n",
    "            result.append(0)\n",
    "            \n",
    "result = np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.253674300Z"
    }
   },
   "outputs": [],
   "source": [
    "# the results may vary\n",
    "# C=0.1, gamma=0.01\n",
    "print('Results for the layer 2 (SVM):')\n",
    "print(confusion_matrix(y_test_real,result))\n",
    "print('Accuracy = ', accuracy_score(y_test_real,result))\n",
    "print('F1 Score = ', f1_score(y_test_real,result))\n",
    "print('Precision = ', precision_score(y_test_real,result))\n",
    "print('Recall = ', recall_score(y_test_real,result))\n",
    "print('Matthew corr = ', matthews_corrcoef(y_test_real,result))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Export the test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if EXPORT_DATASETS:\n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_layer1.shape[1] + 1)]\n",
    "    X1_test = pd.DataFrame(data=X_test_layer1, columns=column_names)\n",
    "    X1_test.to_csv('KB Process/NSL-KDD Encoded Datasets/before_pca/X_test_l1.txt', index=False)\n",
    "    \n",
    "    column_names = [f'PC{i}' for i in range(1, X_test_layer2.shape[1] + 1)]\n",
    "    X2_test = pd.DataFrame(data=X_test_layer2, columns=column_names)\n",
    "    X2_test.to_csv('KB Process/NSL-KDD Encoded Datasets/before_pca/X_test_l2.txt', index=False)\n",
    "    \n",
    "    np.save('KB Process/NSL-KDD Encoded Datasets/before_pca/y_test', y_test_real)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.255668700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate seen and unseen attack categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.258125300Z"
    }
   },
   "outputs": [],
   "source": [
    "# load testset\n",
    "df_test = pd.read_csv('KB Process/NSL-KDD Original Datasets\\KDDTest+.txt', sep=\",\", header=None)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "y_test = df_test['label']\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "\n",
    "df_test_original = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate and plot a synthetic imbalanced classification dataset\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "X, y = copy.deepcopy(df_test), copy.deepcopy(y_test)\n",
    "\n",
    "counter = Counter(y)\n",
    "\n",
    "# scatter plot of examples by class label\n",
    "for label, _ in counter.items():\n",
    "    row_ix = where(y == label)[0]\n",
    "    pyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\n",
    "    \n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.259135800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if EXPORT_DATASETS:\n",
    "    df_test_original.to_csv('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDTest+', index=False)\n",
    "    np.save('KB Process/NSL-KDD Encoded Datasets/before_pca/KDDTest+_targets', y_test)\n",
    "    \n",
    "df_test_original"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.261122800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.263223600Z"
    }
   },
   "outputs": [],
   "source": [
    "new_attack = []\n",
    "for i in df_test_original['label'].value_counts().index.tolist()[1:]:\n",
    "    if i not in df_train_original['label'].value_counts().index.tolist()[1:]:\n",
    "        new_attack.append(i)\n",
    "        \n",
    "new_attack.sort()\n",
    "new_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.265222500Z"
    }
   },
   "outputs": [],
   "source": [
    "index_of_new_attacks = []\n",
    "\n",
    "for i in range(len(df_test_original)):\n",
    "    if df_test_original['label'][i] in new_attack:\n",
    "        index_of_new_attacks.append(df_test_original.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(index_of_new_attacks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.267222500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.268288600Z"
    }
   },
   "outputs": [],
   "source": [
    "new_attack.append('normal')\n",
    "new_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.270375700Z"
    }
   },
   "outputs": [],
   "source": [
    "index_of_old_attacks = []\n",
    "\n",
    "for i in range(len(df_test_original)):\n",
    "    if df_test_original['label'][i] not in new_attack:\n",
    "        index_of_old_attacks.append(df_test_original.index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.272375200Z"
    }
   },
   "outputs": [],
   "source": [
    "len(index_of_old_attacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.275072600Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of new attacks in the test set: ', result[index_of_new_attacks].shape[0])\n",
    "print('Number of new attacks detected by the classifiers: ', result[index_of_new_attacks].sum())\n",
    "print('Proportion of new attacks detected: ', result[index_of_new_attacks].sum()/result[index_of_new_attacks].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.276131400Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Number of old attacks in the test set: ', result[index_of_old_attacks].shape[0])\n",
    "print('Number of old attacks detected by the classifiers: ', result[index_of_old_attacks].sum())\n",
    "print('Proportion of old attacks detected: ', result[index_of_old_attacks].sum()/result[index_of_old_attacks].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate single attack types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.278145900Z"
    }
   },
   "outputs": [],
   "source": [
    "# load test set\n",
    "df_test = pd.read_csv('KB Process/NSL-KDD Original Datasets/KDDTest+.txt', sep=\",\", header=None)\n",
    "df_test = df_test[df_test.columns[:-1]]\n",
    "df_test.columns = titles.to_list()\n",
    "y_test = df_test['label']\n",
    "df_test = df_test.drop(['num_outbound_cmds'],axis=1)\n",
    "df_test_original = df_test\n",
    "df = df_test_original\n",
    "\n",
    "dos_index = df.index[(df['label'].isin(dos_attacks))].tolist()\n",
    "probe_index = df.index[(df['label'].isin(probe_attacks))].tolist()\n",
    "r2l_index = df.index[(df['label'].isin(r2l_attacks))].tolist()\n",
    "u2r_index = df.index[(df['label'].isin(u2r_attacks))].tolist()\n",
    "\n",
    "print('Evaluation split into single attack type:')\n",
    "print(\"Number of dos attacks: \", result[dos_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[dos_index].sum())\n",
    "print(\"Ratio of detection: \", result[dos_index].sum()/result[dos_index].shape[0])\n",
    "\n",
    "print(\"Number of probe attacks: \", result[probe_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[probe_index].sum())\n",
    "print(\"Ratio of detection: \", result[probe_index].sum()/result[probe_index].shape[0])\n",
    "\n",
    "print(\"Number of r2l attacks: \", result[r2l_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[r2l_index].sum())\n",
    "print(\"Ratio of detection: \", result[r2l_index].sum()/result[r2l_index].shape[0])\n",
    "\n",
    "print(\"Number of u2r attacks: \", result[u2r_index].shape[0])\n",
    "print(\"Number of detected attacks: \", result[u2r_index].sum())\n",
    "print(\"Ratio of detection: \", result[u2r_index].sum()/result[u2r_index].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export one hot encoders\n",
    "if EXPORT_ENCODERS:\n",
    "    joblib.dump(ohe, 'KB Process/Required Files/one_hot_encoders/OneHotEncoder_l1.pkl')\n",
    "    joblib.dump(ohe2, 'KB Process/Required Files/one_hot_encoders/OneHotEncoder_l2.pkl')\n",
    "    joblib.dump(scaler1, 'KB Process/Required Files/scalers/Scaler_l1.pkl')\n",
    "    joblib.dump(scaler2, 'KB Process/Required Files/scalers/Scaler_l2.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-11T10:30:24.280150500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
